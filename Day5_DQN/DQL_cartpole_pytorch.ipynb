{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0, reward: 9.0, avg: 9.0\n",
      "episode: 1, reward: 10.0, avg: 9.5\n",
      "episode: 2, reward: 8.0, avg: 9.0\n",
      "episode: 3, reward: 9.0, avg: 9.0\n",
      "episode: 4, reward: 8.0, avg: 8.8\n",
      "episode: 5, reward: 10.0, avg: 9.0\n",
      "episode: 6, reward: 10.0, avg: 9.1\n",
      "episode: 7, reward: 10.0, avg: 9.2\n",
      "episode: 8, reward: 10.0, avg: 9.3\n",
      "episode: 9, reward: 10.0, avg: 9.4\n",
      "episode: 10, reward: 10.0, avg: 9.5\n",
      "episode: 11, reward: 10.0, avg: 9.5\n",
      "episode: 12, reward: 9.0, avg: 9.5\n",
      "episode: 13, reward: 11.0, avg: 9.6\n",
      "episode: 14, reward: 9.0, avg: 9.5\n",
      "episode: 15, reward: 9.0, avg: 9.5\n",
      "episode: 16, reward: 9.0, avg: 9.5\n",
      "episode: 17, reward: 9.0, avg: 9.4\n",
      "episode: 18, reward: 10.0, avg: 9.5\n",
      "episode: 19, reward: 10.0, avg: 9.5\n",
      "episode: 20, reward: 10.0, avg: 9.5\n",
      "episode: 21, reward: 9.0, avg: 9.5\n",
      "episode: 22, reward: 9.0, avg: 9.5\n",
      "episode: 23, reward: 10.0, avg: 9.5\n",
      "episode: 24, reward: 10.0, avg: 9.5\n",
      "episode: 25, reward: 8.0, avg: 9.5\n",
      "episode: 26, reward: 9.0, avg: 9.4\n",
      "episode: 27, reward: 9.0, avg: 9.4\n",
      "episode: 28, reward: 9.0, avg: 9.4\n",
      "episode: 29, reward: 8.0, avg: 9.4\n",
      "episode: 30, reward: 9.0, avg: 9.4\n",
      "episode: 31, reward: 9.0, avg: 9.3\n",
      "episode: 32, reward: 10.0, avg: 9.4\n",
      "episode: 33, reward: 8.0, avg: 9.3\n",
      "episode: 34, reward: 9.0, avg: 9.3\n",
      "episode: 35, reward: 10.0, avg: 9.3\n",
      "episode: 36, reward: 10.0, avg: 9.4\n",
      "episode: 37, reward: 10.0, avg: 9.4\n",
      "episode: 38, reward: 8.0, avg: 9.3\n",
      "episode: 39, reward: 10.0, avg: 9.3\n",
      "episode: 40, reward: 10.0, avg: 9.4\n",
      "episode: 41, reward: 10.0, avg: 9.4\n",
      "episode: 42, reward: 9.0, avg: 9.4\n",
      "episode: 43, reward: 9.0, avg: 9.4\n",
      "episode: 44, reward: 9.0, avg: 9.4\n",
      "episode: 45, reward: 9.0, avg: 9.3\n",
      "episode: 46, reward: 9.0, avg: 9.3\n",
      "episode: 47, reward: 8.0, avg: 9.3\n",
      "episode: 48, reward: 10.0, avg: 9.3\n",
      "episode: 49, reward: 10.0, avg: 9.3\n",
      "episode: 50, reward: 9.0, avg: 9.3\n",
      "episode: 51, reward: 9.0, avg: 9.3\n",
      "episode: 52, reward: 9.0, avg: 9.3\n",
      "episode: 53, reward: 9.0, avg: 9.3\n",
      "episode: 54, reward: 10.0, avg: 9.3\n",
      "episode: 55, reward: 9.0, avg: 9.3\n",
      "episode: 56, reward: 10.0, avg: 9.3\n",
      "episode: 57, reward: 9.0, avg: 9.3\n",
      "episode: 58, reward: 9.0, avg: 9.3\n",
      "episode: 59, reward: 10.0, avg: 9.3\n",
      "episode: 60, reward: 9.0, avg: 9.3\n",
      "episode: 61, reward: 11.0, avg: 9.4\n",
      "episode: 62, reward: 8.0, avg: 9.3\n",
      "episode: 63, reward: 9.0, avg: 9.3\n",
      "episode: 64, reward: 9.0, avg: 9.3\n",
      "episode: 65, reward: 11.0, avg: 9.3\n",
      "episode: 66, reward: 9.0, avg: 9.3\n",
      "episode: 67, reward: 9.0, avg: 9.3\n",
      "episode: 68, reward: 9.0, avg: 9.3\n",
      "episode: 69, reward: 10.0, avg: 9.3\n",
      "episode: 70, reward: 10.0, avg: 9.4\n",
      "episode: 71, reward: 10.0, avg: 9.4\n",
      "episode: 72, reward: 11.0, avg: 9.4\n",
      "episode: 73, reward: 9.0, avg: 9.4\n",
      "episode: 74, reward: 8.0, avg: 9.4\n",
      "episode: 75, reward: 8.0, avg: 9.3\n",
      "episode: 76, reward: 8.0, avg: 9.3\n",
      "episode: 77, reward: 13.0, avg: 9.4\n",
      "episode: 78, reward: 10.0, avg: 9.4\n",
      "episode: 79, reward: 9.0, avg: 9.4\n",
      "episode: 80, reward: 10.0, avg: 9.4\n",
      "episode: 81, reward: 9.0, avg: 9.4\n",
      "episode: 82, reward: 10.0, avg: 9.4\n",
      "episode: 83, reward: 11.0, avg: 9.4\n",
      "episode: 84, reward: 8.0, avg: 9.4\n",
      "episode: 85, reward: 9.0, avg: 9.4\n",
      "episode: 86, reward: 10.0, avg: 9.4\n",
      "episode: 87, reward: 10.0, avg: 9.4\n",
      "episode: 88, reward: 8.0, avg: 9.4\n",
      "episode: 89, reward: 9.0, avg: 9.4\n",
      "episode: 90, reward: 10.0, avg: 9.4\n",
      "episode: 91, reward: 10.0, avg: 9.4\n",
      "episode: 92, reward: 10.0, avg: 9.4\n",
      "episode: 93, reward: 10.0, avg: 9.4\n",
      "episode: 94, reward: 9.0, avg: 9.4\n",
      "episode: 95, reward: 10.0, avg: 9.4\n",
      "episode: 96, reward: 10.0, avg: 9.4\n",
      "episode: 97, reward: 9.0, avg: 9.4\n",
      "episode: 98, reward: 9.0, avg: 9.4\n",
      "episode: 99, reward: 10.0, avg: 9.4\n",
      "episode: 100, reward: 10.0, avg: 9.4\n",
      "episode: 101, reward: 8.0, avg: 9.4\n",
      "episode: 102, reward: 8.0, avg: 9.4\n",
      "episode: 103, reward: 10.0, avg: 9.4\n",
      "episode: 104, reward: 10.0, avg: 9.4\n",
      "episode: 105, reward: 9.0, avg: 9.4\n",
      "episode: 106, reward: 9.0, avg: 9.4\n",
      "episode: 107, reward: 10.0, avg: 9.4\n",
      "episode: 108, reward: 10.0, avg: 9.4\n",
      "episode: 109, reward: 9.0, avg: 9.4\n",
      "episode: 110, reward: 8.0, avg: 9.4\n",
      "episode: 111, reward: 10.0, avg: 9.4\n",
      "episode: 112, reward: 10.0, avg: 9.4\n",
      "episode: 113, reward: 9.0, avg: 9.4\n",
      "episode: 114, reward: 9.0, avg: 9.4\n",
      "episode: 115, reward: 9.0, avg: 9.4\n",
      "episode: 116, reward: 9.0, avg: 9.4\n",
      "episode: 117, reward: 9.0, avg: 9.4\n",
      "episode: 118, reward: 10.0, avg: 9.4\n",
      "episode: 119, reward: 9.0, avg: 9.4\n",
      "episode: 120, reward: 10.0, avg: 9.4\n",
      "episode: 121, reward: 9.0, avg: 9.4\n",
      "episode: 122, reward: 8.0, avg: 9.3\n",
      "episode: 123, reward: 9.0, avg: 9.3\n",
      "episode: 124, reward: 9.0, avg: 9.3\n",
      "episode: 125, reward: 9.0, avg: 9.3\n",
      "episode: 126, reward: 9.0, avg: 9.3\n",
      "episode: 127, reward: 10.0, avg: 9.3\n",
      "episode: 128, reward: 11.0, avg: 9.4\n",
      "episode: 129, reward: 10.0, avg: 9.4\n",
      "episode: 130, reward: 9.0, avg: 9.4\n",
      "episode: 131, reward: 10.0, avg: 9.4\n",
      "episode: 132, reward: 9.0, avg: 9.4\n",
      "episode: 133, reward: 9.0, avg: 9.4\n",
      "episode: 134, reward: 10.0, avg: 9.4\n",
      "episode: 135, reward: 10.0, avg: 9.4\n",
      "episode: 136, reward: 9.0, avg: 9.4\n",
      "episode: 137, reward: 10.0, avg: 9.4\n",
      "episode: 138, reward: 9.0, avg: 9.4\n",
      "episode: 139, reward: 10.0, avg: 9.4\n",
      "episode: 140, reward: 10.0, avg: 9.4\n",
      "episode: 141, reward: 10.0, avg: 9.4\n",
      "episode: 142, reward: 10.0, avg: 9.4\n",
      "episode: 143, reward: 8.0, avg: 9.4\n",
      "episode: 144, reward: 10.0, avg: 9.4\n",
      "episode: 145, reward: 12.0, avg: 9.4\n",
      "episode: 146, reward: 10.0, avg: 9.5\n",
      "episode: 147, reward: 10.0, avg: 9.5\n",
      "episode: 148, reward: 11.0, avg: 9.5\n",
      "episode: 149, reward: 10.0, avg: 9.5\n",
      "episode: 150, reward: 9.0, avg: 9.5\n",
      "episode: 151, reward: 9.0, avg: 9.5\n",
      "episode: 152, reward: 10.0, avg: 9.5\n",
      "episode: 153, reward: 9.0, avg: 9.5\n",
      "episode: 154, reward: 10.0, avg: 9.5\n",
      "episode: 155, reward: 10.0, avg: 9.5\n",
      "episode: 156, reward: 9.0, avg: 9.5\n",
      "episode: 157, reward: 10.0, avg: 9.5\n",
      "episode: 158, reward: 8.0, avg: 9.5\n",
      "episode: 159, reward: 10.0, avg: 9.5\n",
      "episode: 160, reward: 10.0, avg: 9.5\n",
      "episode: 161, reward: 10.0, avg: 9.5\n",
      "episode: 162, reward: 10.0, avg: 9.5\n",
      "episode: 163, reward: 10.0, avg: 9.5\n",
      "episode: 164, reward: 8.0, avg: 9.5\n",
      "episode: 165, reward: 10.0, avg: 9.5\n",
      "episode: 166, reward: 9.0, avg: 9.5\n",
      "episode: 167, reward: 9.0, avg: 9.5\n",
      "episode: 168, reward: 10.0, avg: 9.5\n",
      "episode: 169, reward: 9.0, avg: 9.5\n",
      "episode: 170, reward: 10.0, avg: 9.5\n",
      "episode: 171, reward: 8.0, avg: 9.5\n",
      "episode: 172, reward: 9.0, avg: 9.5\n",
      "episode: 173, reward: 10.0, avg: 9.5\n",
      "episode: 174, reward: 9.0, avg: 9.5\n",
      "episode: 175, reward: 9.0, avg: 9.5\n",
      "episode: 176, reward: 9.0, avg: 9.5\n",
      "episode: 177, reward: 11.0, avg: 9.5\n",
      "episode: 178, reward: 9.0, avg: 9.5\n",
      "episode: 179, reward: 10.0, avg: 9.5\n",
      "episode: 180, reward: 10.0, avg: 9.5\n",
      "episode: 181, reward: 10.0, avg: 9.5\n",
      "episode: 182, reward: 10.0, avg: 9.5\n",
      "episode: 183, reward: 8.0, avg: 9.5\n",
      "episode: 184, reward: 9.0, avg: 9.5\n",
      "episode: 185, reward: 10.0, avg: 9.5\n",
      "episode: 186, reward: 10.0, avg: 9.5\n",
      "episode: 187, reward: 8.0, avg: 9.5\n",
      "episode: 188, reward: 8.0, avg: 9.5\n",
      "episode: 189, reward: 9.0, avg: 9.5\n",
      "episode: 190, reward: 10.0, avg: 9.5\n",
      "episode: 191, reward: 10.0, avg: 9.5\n",
      "episode: 192, reward: 9.0, avg: 9.5\n",
      "episode: 193, reward: 9.0, avg: 9.4\n",
      "episode: 194, reward: 9.0, avg: 9.4\n",
      "episode: 195, reward: 8.0, avg: 9.4\n",
      "episode: 196, reward: 10.0, avg: 9.4\n",
      "episode: 197, reward: 9.0, avg: 9.4\n",
      "episode: 198, reward: 9.0, avg: 9.4\n",
      "episode: 199, reward: 9.0, avg: 9.4\n",
      "episode: 200, reward: 9.0, avg: 9.4\n",
      "episode: 201, reward: 9.0, avg: 9.4\n",
      "episode: 202, reward: 10.0, avg: 9.4\n",
      "episode: 203, reward: 9.0, avg: 9.4\n",
      "episode: 204, reward: 9.0, avg: 9.4\n",
      "episode: 205, reward: 10.0, avg: 9.4\n",
      "episode: 206, reward: 9.0, avg: 9.4\n",
      "episode: 207, reward: 12.0, avg: 9.4\n",
      "episode: 208, reward: 10.0, avg: 9.4\n",
      "episode: 209, reward: 10.0, avg: 9.5\n",
      "episode: 210, reward: 9.0, avg: 9.5\n",
      "episode: 211, reward: 9.0, avg: 9.5\n",
      "episode: 212, reward: 11.0, avg: 9.5\n",
      "episode: 213, reward: 10.0, avg: 9.5\n",
      "episode: 214, reward: 9.0, avg: 9.5\n",
      "episode: 215, reward: 10.0, avg: 9.5\n",
      "episode: 216, reward: 10.0, avg: 9.5\n",
      "episode: 217, reward: 9.0, avg: 9.5\n",
      "episode: 218, reward: 8.0, avg: 9.5\n",
      "episode: 219, reward: 9.0, avg: 9.5\n",
      "episode: 220, reward: 10.0, avg: 9.5\n",
      "episode: 221, reward: 10.0, avg: 9.5\n",
      "episode: 222, reward: 9.0, avg: 9.5\n",
      "episode: 223, reward: 9.0, avg: 9.5\n",
      "episode: 224, reward: 9.0, avg: 9.5\n",
      "episode: 225, reward: 8.0, avg: 9.5\n",
      "episode: 226, reward: 11.0, avg: 9.5\n",
      "episode: 227, reward: 10.0, avg: 9.5\n",
      "episode: 228, reward: 9.0, avg: 9.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 229, reward: 21.0, avg: 9.6\n",
      "episode: 230, reward: 10.0, avg: 9.6\n",
      "episode: 231, reward: 11.0, avg: 9.6\n",
      "episode: 232, reward: 11.0, avg: 9.6\n",
      "episode: 233, reward: 9.0, avg: 9.6\n",
      "episode: 234, reward: 9.0, avg: 9.6\n",
      "episode: 235, reward: 11.0, avg: 9.6\n",
      "episode: 236, reward: 10.0, avg: 9.7\n",
      "episode: 237, reward: 9.0, avg: 9.6\n",
      "episode: 238, reward: 11.0, avg: 9.7\n",
      "episode: 239, reward: 9.0, avg: 9.7\n",
      "episode: 240, reward: 12.0, avg: 9.7\n",
      "episode: 241, reward: 13.0, avg: 9.7\n",
      "episode: 242, reward: 12.0, avg: 9.7\n",
      "episode: 243, reward: 9.0, avg: 9.7\n",
      "episode: 244, reward: 10.0, avg: 9.7\n",
      "episode: 245, reward: 11.0, avg: 9.7\n",
      "episode: 246, reward: 13.0, avg: 9.8\n",
      "episode: 247, reward: 14.0, avg: 9.8\n",
      "episode: 248, reward: 10.0, avg: 9.8\n",
      "episode: 249, reward: 11.0, avg: 9.8\n",
      "episode: 250, reward: 11.0, avg: 9.8\n",
      "episode: 251, reward: 10.0, avg: 9.8\n",
      "episode: 252, reward: 10.0, avg: 9.8\n",
      "episode: 253, reward: 11.0, avg: 9.8\n",
      "episode: 254, reward: 11.0, avg: 9.8\n",
      "episode: 255, reward: 10.0, avg: 9.8\n",
      "episode: 256, reward: 8.0, avg: 9.8\n",
      "episode: 257, reward: 9.0, avg: 9.8\n",
      "episode: 258, reward: 11.0, avg: 9.9\n",
      "episode: 259, reward: 23.0, avg: 10.0\n",
      "episode: 260, reward: 9.0, avg: 10.0\n",
      "episode: 261, reward: 9.0, avg: 10.0\n",
      "episode: 262, reward: 9.0, avg: 10.0\n",
      "episode: 263, reward: 17.0, avg: 10.0\n",
      "episode: 264, reward: 9.0, avg: 10.0\n",
      "episode: 265, reward: 9.0, avg: 10.0\n",
      "episode: 266, reward: 9.0, avg: 10.0\n",
      "episode: 267, reward: 10.0, avg: 10.0\n",
      "episode: 268, reward: 12.0, avg: 10.1\n",
      "episode: 269, reward: 10.0, avg: 10.1\n",
      "episode: 270, reward: 9.0, avg: 10.1\n",
      "episode: 271, reward: 10.0, avg: 10.1\n",
      "episode: 272, reward: 9.0, avg: 10.1\n",
      "episode: 273, reward: 9.0, avg: 10.1\n",
      "episode: 274, reward: 12.0, avg: 10.1\n",
      "episode: 275, reward: 12.0, avg: 10.1\n",
      "episode: 276, reward: 12.0, avg: 10.2\n",
      "episode: 277, reward: 9.0, avg: 10.1\n",
      "episode: 278, reward: 12.0, avg: 10.2\n",
      "episode: 279, reward: 11.0, avg: 10.2\n",
      "episode: 280, reward: 11.0, avg: 10.2\n",
      "episode: 281, reward: 12.0, avg: 10.2\n",
      "episode: 282, reward: 10.0, avg: 10.2\n",
      "episode: 283, reward: 10.0, avg: 10.2\n",
      "episode: 284, reward: 9.0, avg: 10.2\n",
      "episode: 285, reward: 9.0, avg: 10.2\n",
      "episode: 286, reward: 10.0, avg: 10.2\n",
      "episode: 287, reward: 12.0, avg: 10.3\n",
      "episode: 288, reward: 11.0, avg: 10.3\n",
      "episode: 289, reward: 8.0, avg: 10.3\n",
      "episode: 290, reward: 9.0, avg: 10.3\n",
      "episode: 291, reward: 9.0, avg: 10.3\n",
      "episode: 292, reward: 10.0, avg: 10.3\n",
      "episode: 293, reward: 10.0, avg: 10.3\n",
      "episode: 294, reward: 8.0, avg: 10.3\n",
      "episode: 295, reward: 10.0, avg: 10.3\n",
      "episode: 296, reward: 9.0, avg: 10.3\n",
      "episode: 297, reward: 9.0, avg: 10.3\n",
      "episode: 298, reward: 11.0, avg: 10.3\n",
      "episode: 299, reward: 10.0, avg: 10.3\n",
      "episode: 300, reward: 12.0, avg: 10.3\n",
      "episode: 301, reward: 10.0, avg: 10.3\n",
      "episode: 302, reward: 12.0, avg: 10.4\n",
      "episode: 303, reward: 12.0, avg: 10.4\n",
      "episode: 304, reward: 11.0, avg: 10.4\n",
      "episode: 305, reward: 9.0, avg: 10.4\n",
      "episode: 306, reward: 12.0, avg: 10.4\n",
      "episode: 307, reward: 9.0, avg: 10.4\n",
      "episode: 308, reward: 10.0, avg: 10.4\n",
      "episode: 309, reward: 9.0, avg: 10.4\n",
      "episode: 310, reward: 10.0, avg: 10.4\n",
      "episode: 311, reward: 10.0, avg: 10.4\n",
      "episode: 312, reward: 9.0, avg: 10.4\n",
      "episode: 313, reward: 10.0, avg: 10.4\n",
      "episode: 314, reward: 9.0, avg: 10.4\n",
      "episode: 315, reward: 9.0, avg: 10.4\n",
      "episode: 316, reward: 10.0, avg: 10.4\n",
      "episode: 317, reward: 10.0, avg: 10.4\n",
      "episode: 318, reward: 11.0, avg: 10.4\n",
      "episode: 319, reward: 9.0, avg: 10.4\n",
      "episode: 320, reward: 8.0, avg: 10.4\n",
      "episode: 321, reward: 9.0, avg: 10.4\n",
      "episode: 322, reward: 9.0, avg: 10.4\n",
      "episode: 323, reward: 11.0, avg: 10.4\n",
      "episode: 324, reward: 10.0, avg: 10.4\n",
      "episode: 325, reward: 11.0, avg: 10.5\n",
      "episode: 326, reward: 9.0, avg: 10.4\n",
      "episode: 327, reward: 9.0, avg: 10.4\n",
      "episode: 328, reward: 12.0, avg: 10.5\n",
      "episode: 329, reward: 10.0, avg: 10.3\n",
      "episode: 330, reward: 12.0, avg: 10.4\n",
      "episode: 331, reward: 9.0, avg: 10.3\n",
      "episode: 332, reward: 8.0, avg: 10.3\n",
      "episode: 333, reward: 9.0, avg: 10.3\n",
      "episode: 334, reward: 11.0, avg: 10.3\n",
      "episode: 335, reward: 10.0, avg: 10.3\n",
      "episode: 336, reward: 10.0, avg: 10.3\n",
      "episode: 337, reward: 12.0, avg: 10.4\n",
      "episode: 338, reward: 10.0, avg: 10.3\n",
      "episode: 339, reward: 12.0, avg: 10.4\n",
      "episode: 340, reward: 10.0, avg: 10.4\n",
      "episode: 341, reward: 9.0, avg: 10.3\n",
      "episode: 342, reward: 10.0, avg: 10.3\n",
      "episode: 343, reward: 12.0, avg: 10.3\n",
      "episode: 344, reward: 11.0, avg: 10.3\n",
      "episode: 345, reward: 25.0, avg: 10.5\n",
      "episode: 346, reward: 10.0, avg: 10.4\n",
      "episode: 347, reward: 10.0, avg: 10.4\n",
      "episode: 348, reward: 13.0, avg: 10.4\n",
      "episode: 349, reward: 16.0, avg: 10.5\n",
      "episode: 350, reward: 9.0, avg: 10.5\n",
      "episode: 351, reward: 11.0, avg: 10.5\n",
      "episode: 352, reward: 9.0, avg: 10.5\n",
      "episode: 353, reward: 10.0, avg: 10.5\n",
      "episode: 354, reward: 10.0, avg: 10.4\n",
      "episode: 355, reward: 10.0, avg: 10.4\n",
      "episode: 356, reward: 9.0, avg: 10.5\n",
      "episode: 357, reward: 8.0, avg: 10.4\n",
      "episode: 358, reward: 11.0, avg: 10.4\n",
      "episode: 359, reward: 20.0, avg: 10.4\n",
      "episode: 360, reward: 9.0, avg: 10.4\n",
      "episode: 361, reward: 9.0, avg: 10.4\n",
      "episode: 362, reward: 10.0, avg: 10.4\n",
      "episode: 363, reward: 11.0, avg: 10.4\n",
      "episode: 364, reward: 8.0, avg: 10.4\n",
      "episode: 365, reward: 18.0, avg: 10.4\n",
      "episode: 366, reward: 25.0, avg: 10.6\n",
      "episode: 367, reward: 14.0, avg: 10.7\n",
      "episode: 368, reward: 22.0, avg: 10.8\n",
      "episode: 369, reward: 9.0, avg: 10.7\n",
      "episode: 370, reward: 10.0, avg: 10.8\n",
      "episode: 371, reward: 9.0, avg: 10.7\n",
      "episode: 372, reward: 10.0, avg: 10.8\n",
      "episode: 373, reward: 14.0, avg: 10.8\n",
      "episode: 374, reward: 10.0, avg: 10.8\n",
      "episode: 375, reward: 16.0, avg: 10.8\n",
      "episode: 376, reward: 10.0, avg: 10.8\n",
      "episode: 377, reward: 11.0, avg: 10.8\n",
      "episode: 378, reward: 25.0, avg: 10.9\n",
      "episode: 379, reward: 9.0, avg: 10.9\n",
      "episode: 380, reward: 11.0, avg: 10.9\n",
      "episode: 381, reward: 9.0, avg: 10.9\n",
      "episode: 382, reward: 11.0, avg: 10.9\n",
      "episode: 383, reward: 14.0, avg: 10.9\n",
      "episode: 384, reward: 11.0, avg: 11.0\n",
      "episode: 385, reward: 15.0, avg: 11.0\n",
      "episode: 386, reward: 9.0, avg: 11.0\n",
      "episode: 387, reward: 9.0, avg: 11.0\n",
      "episode: 388, reward: 9.0, avg: 11.0\n",
      "episode: 389, reward: 10.0, avg: 11.0\n",
      "episode: 390, reward: 8.0, avg: 11.0\n",
      "episode: 391, reward: 11.0, avg: 11.0\n",
      "episode: 392, reward: 11.0, avg: 11.0\n",
      "episode: 393, reward: 10.0, avg: 11.0\n",
      "episode: 394, reward: 10.0, avg: 11.0\n",
      "episode: 395, reward: 19.0, avg: 11.1\n",
      "episode: 396, reward: 10.0, avg: 11.1\n",
      "episode: 397, reward: 11.0, avg: 11.2\n",
      "episode: 398, reward: 9.0, avg: 11.1\n",
      "episode: 399, reward: 10.0, avg: 11.1\n",
      "episode: 400, reward: 9.0, avg: 11.1\n",
      "episode: 401, reward: 8.0, avg: 11.1\n",
      "episode: 402, reward: 12.0, avg: 11.1\n",
      "episode: 403, reward: 10.0, avg: 11.1\n",
      "episode: 404, reward: 10.0, avg: 11.1\n",
      "episode: 405, reward: 10.0, avg: 11.1\n",
      "episode: 406, reward: 11.0, avg: 11.1\n",
      "episode: 407, reward: 10.0, avg: 11.1\n",
      "episode: 408, reward: 10.0, avg: 11.1\n",
      "episode: 409, reward: 16.0, avg: 11.1\n",
      "episode: 410, reward: 10.0, avg: 11.1\n",
      "episode: 411, reward: 9.0, avg: 11.1\n",
      "episode: 412, reward: 11.0, avg: 11.1\n",
      "episode: 413, reward: 10.0, avg: 11.1\n",
      "episode: 414, reward: 9.0, avg: 11.1\n",
      "episode: 415, reward: 9.0, avg: 11.1\n",
      "episode: 416, reward: 29.0, avg: 11.3\n",
      "episode: 417, reward: 17.0, avg: 11.4\n",
      "episode: 418, reward: 9.0, avg: 11.4\n",
      "episode: 419, reward: 8.0, avg: 11.4\n",
      "episode: 420, reward: 10.0, avg: 11.4\n",
      "episode: 421, reward: 9.0, avg: 11.4\n",
      "episode: 422, reward: 9.0, avg: 11.4\n",
      "episode: 423, reward: 11.0, avg: 11.4\n",
      "episode: 424, reward: 11.0, avg: 11.4\n",
      "episode: 425, reward: 9.0, avg: 11.4\n",
      "episode: 426, reward: 12.0, avg: 11.4\n",
      "episode: 427, reward: 14.0, avg: 11.5\n",
      "episode: 428, reward: 19.0, avg: 11.5\n",
      "episode: 429, reward: 9.0, avg: 11.5\n",
      "episode: 430, reward: 10.0, avg: 11.5\n",
      "episode: 431, reward: 13.0, avg: 11.5\n",
      "episode: 432, reward: 13.0, avg: 11.6\n",
      "episode: 433, reward: 24.0, avg: 11.7\n",
      "episode: 434, reward: 15.0, avg: 11.8\n",
      "episode: 435, reward: 11.0, avg: 11.8\n",
      "episode: 436, reward: 12.0, avg: 11.8\n",
      "episode: 437, reward: 10.0, avg: 11.8\n",
      "episode: 438, reward: 16.0, avg: 11.8\n",
      "episode: 439, reward: 22.0, avg: 11.9\n",
      "episode: 440, reward: 25.0, avg: 12.1\n",
      "episode: 441, reward: 12.0, avg: 12.1\n",
      "episode: 442, reward: 11.0, avg: 12.1\n",
      "episode: 443, reward: 14.0, avg: 12.2\n",
      "episode: 444, reward: 25.0, avg: 12.3\n",
      "episode: 445, reward: 21.0, avg: 12.3\n",
      "episode: 446, reward: 12.0, avg: 12.3\n",
      "episode: 447, reward: 24.0, avg: 12.4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 448, reward: 42.0, avg: 12.7\n",
      "episode: 449, reward: 13.0, avg: 12.7\n",
      "episode: 450, reward: 23.0, avg: 12.8\n",
      "episode: 451, reward: 34.0, avg: 13.1\n",
      "episode: 452, reward: 16.0, avg: 13.1\n",
      "episode: 453, reward: 16.0, avg: 13.2\n",
      "episode: 454, reward: 39.0, avg: 13.5\n",
      "episode: 455, reward: 16.0, avg: 13.5\n",
      "episode: 456, reward: 19.0, avg: 13.6\n",
      "episode: 457, reward: 25.0, avg: 13.8\n",
      "episode: 458, reward: 11.0, avg: 13.8\n",
      "episode: 459, reward: 21.0, avg: 13.8\n",
      "episode: 460, reward: 19.0, avg: 13.9\n",
      "episode: 461, reward: 27.0, avg: 14.1\n",
      "episode: 462, reward: 17.0, avg: 14.2\n",
      "episode: 463, reward: 50.0, avg: 14.6\n",
      "episode: 464, reward: 23.0, avg: 14.7\n",
      "episode: 465, reward: 32.0, avg: 14.8\n",
      "episode: 466, reward: 10.0, avg: 14.7\n",
      "episode: 467, reward: 21.0, avg: 14.8\n",
      "episode: 468, reward: 15.0, avg: 14.7\n",
      "episode: 469, reward: 20.0, avg: 14.8\n",
      "episode: 470, reward: 15.0, avg: 14.8\n",
      "episode: 471, reward: 15.0, avg: 14.9\n",
      "episode: 472, reward: 16.0, avg: 15.0\n",
      "episode: 473, reward: 17.0, avg: 15.0\n",
      "episode: 474, reward: 18.0, avg: 15.1\n",
      "episode: 475, reward: 55.0, avg: 15.5\n",
      "episode: 476, reward: 17.0, avg: 15.5\n",
      "episode: 477, reward: 25.0, avg: 15.7\n",
      "episode: 478, reward: 44.0, avg: 15.9\n",
      "episode: 479, reward: 37.0, avg: 16.1\n",
      "episode: 480, reward: 17.0, avg: 16.2\n",
      "episode: 481, reward: 36.0, avg: 16.5\n",
      "episode: 482, reward: 27.0, avg: 16.6\n",
      "episode: 483, reward: 32.0, avg: 16.8\n",
      "episode: 484, reward: 25.0, avg: 17.0\n",
      "episode: 485, reward: 21.0, avg: 17.0\n",
      "episode: 486, reward: 22.0, avg: 17.1\n",
      "episode: 487, reward: 16.0, avg: 17.2\n",
      "episode: 488, reward: 29.0, avg: 17.4\n",
      "episode: 489, reward: 32.0, avg: 17.6\n",
      "episode: 490, reward: 32.0, avg: 17.9\n",
      "episode: 491, reward: 36.0, avg: 18.1\n",
      "episode: 492, reward: 31.0, avg: 18.3\n",
      "episode: 493, reward: 28.0, avg: 18.5\n",
      "episode: 494, reward: 33.0, avg: 18.7\n",
      "episode: 495, reward: 45.0, avg: 19.0\n",
      "episode: 496, reward: 35.0, avg: 19.2\n",
      "episode: 497, reward: 32.0, avg: 19.5\n",
      "episode: 498, reward: 40.0, avg: 19.8\n",
      "episode: 499, reward: 47.0, avg: 20.1\n",
      "episode: 500, reward: 37.0, avg: 20.4\n",
      "episode: 501, reward: 31.0, avg: 20.6\n",
      "episode: 502, reward: 34.0, avg: 20.9\n",
      "episode: 503, reward: 25.0, avg: 21.0\n",
      "episode: 504, reward: 26.0, avg: 21.2\n",
      "episode: 505, reward: 32.0, avg: 21.4\n",
      "episode: 506, reward: 35.0, avg: 21.6\n",
      "episode: 507, reward: 37.0, avg: 21.9\n",
      "episode: 508, reward: 25.0, avg: 22.1\n",
      "episode: 509, reward: 48.0, avg: 22.4\n",
      "episode: 510, reward: 108.0, avg: 23.4\n",
      "episode: 511, reward: 41.0, avg: 23.7\n",
      "episode: 512, reward: 66.0, avg: 24.2\n",
      "episode: 513, reward: 22.0, avg: 24.4\n",
      "episode: 514, reward: 136.0, avg: 25.6\n",
      "episode: 515, reward: 30.0, avg: 25.8\n",
      "episode: 516, reward: 162.0, avg: 27.2\n",
      "episode: 517, reward: 111.0, avg: 28.1\n",
      "episode: 518, reward: 100.0, avg: 29.0\n",
      "episode: 519, reward: 199.0, avg: 30.9\n",
      "episode: 520, reward: 182.0, avg: 32.6\n",
      "episode: 521, reward: 155.0, avg: 34.1\n",
      "episode: 522, reward: 162.0, avg: 35.6\n",
      "episode: 523, reward: 141.0, avg: 36.9\n",
      "episode: 524, reward: 191.0, avg: 38.7\n",
      "episode: 525, reward: 200.0, avg: 40.6\n",
      "episode: 526, reward: 200.0, avg: 42.5\n",
      "episode: 527, reward: 200.0, avg: 44.4\n",
      "episode: 528, reward: 200.0, avg: 46.2\n",
      "episode: 529, reward: 187.0, avg: 48.0\n",
      "episode: 530, reward: 156.0, avg: 49.4\n",
      "episode: 531, reward: 162.0, avg: 50.9\n",
      "episode: 532, reward: 140.0, avg: 52.2\n",
      "episode: 533, reward: 115.0, avg: 53.1\n",
      "episode: 534, reward: 140.0, avg: 54.4\n",
      "episode: 535, reward: 143.0, avg: 55.7\n",
      "episode: 536, reward: 135.0, avg: 56.9\n",
      "episode: 537, reward: 188.0, avg: 58.7\n",
      "episode: 538, reward: 200.0, avg: 60.5\n",
      "episode: 539, reward: 170.0, avg: 62.0\n",
      "episode: 540, reward: 135.0, avg: 63.1\n",
      "episode: 541, reward: 115.0, avg: 64.1\n",
      "episode: 542, reward: 146.0, avg: 65.5\n",
      "episode: 543, reward: 184.0, avg: 67.2\n",
      "episode: 544, reward: 115.0, avg: 68.1\n",
      "episode: 545, reward: 200.0, avg: 69.9\n",
      "episode: 546, reward: 200.0, avg: 71.8\n",
      "episode: 547, reward: 151.0, avg: 73.0\n",
      "episode: 548, reward: 200.0, avg: 74.6\n",
      "episode: 549, reward: 200.0, avg: 76.5\n",
      "episode: 550, reward: 200.0, avg: 78.2\n",
      "episode: 551, reward: 157.0, avg: 79.5\n",
      "episode: 552, reward: 200.0, avg: 81.3\n",
      "episode: 553, reward: 123.0, avg: 82.4\n",
      "episode: 554, reward: 200.0, avg: 84.0\n",
      "episode: 555, reward: 126.0, avg: 85.1\n",
      "episode: 556, reward: 128.0, avg: 86.2\n",
      "episode: 557, reward: 195.0, avg: 87.9\n",
      "episode: 558, reward: 155.0, avg: 89.3\n",
      "episode: 559, reward: 199.0, avg: 91.1\n",
      "episode: 560, reward: 117.0, avg: 92.1\n",
      "episode: 561, reward: 200.0, avg: 93.8\n",
      "episode: 562, reward: 200.0, avg: 95.6\n",
      "episode: 563, reward: 200.0, avg: 97.1\n",
      "episode: 564, reward: 185.0, avg: 98.8\n",
      "episode: 565, reward: 130.0, avg: 99.7\n",
      "episode: 566, reward: 121.0, avg: 100.8\n",
      "episode: 567, reward: 185.0, avg: 102.5\n",
      "episode: 568, reward: 200.0, avg: 104.3\n",
      "episode: 569, reward: 129.0, avg: 105.4\n",
      "episode: 570, reward: 136.0, avg: 106.6\n",
      "episode: 571, reward: 200.0, avg: 108.5\n",
      "episode: 572, reward: 137.0, avg: 109.7\n",
      "episode: 573, reward: 156.0, avg: 111.1\n",
      "episode: 574, reward: 158.0, avg: 112.5\n",
      "episode: 575, reward: 200.0, avg: 113.9\n",
      "episode: 576, reward: 131.0, avg: 115.1\n",
      "episode: 577, reward: 146.0, avg: 116.3\n",
      "episode: 578, reward: 200.0, avg: 117.8\n",
      "episode: 579, reward: 177.0, avg: 119.2\n",
      "episode: 580, reward: 160.0, avg: 120.7\n",
      "episode: 581, reward: 174.0, avg: 122.1\n",
      "episode: 582, reward: 174.0, avg: 123.5\n",
      "episode: 583, reward: 160.0, avg: 124.8\n",
      "episode: 584, reward: 200.0, avg: 126.6\n",
      "episode: 585, reward: 146.0, avg: 127.8\n",
      "episode: 586, reward: 200.0, avg: 129.6\n",
      "episode: 587, reward: 191.0, avg: 131.3\n",
      "episode: 588, reward: 170.0, avg: 132.8\n",
      "episode: 589, reward: 166.0, avg: 134.1\n",
      "episode: 590, reward: 200.0, avg: 135.8\n",
      "episode: 591, reward: 200.0, avg: 137.4\n",
      "episode: 592, reward: 187.0, avg: 139.0\n",
      "episode: 593, reward: 200.0, avg: 140.7\n",
      "episode: 594, reward: 153.0, avg: 141.9\n",
      "episode: 595, reward: 194.0, avg: 143.4\n",
      "episode: 596, reward: 175.0, avg: 144.8\n",
      "episode: 597, reward: 200.0, avg: 146.5\n",
      "episode: 598, reward: 200.0, avg: 148.1\n",
      "episode: 599, reward: 171.0, avg: 149.3\n",
      "episode: 600, reward: 200.0, avg: 150.9\n",
      "episode: 601, reward: 189.0, avg: 152.5\n",
      "episode: 602, reward: 164.0, avg: 153.8\n",
      "episode: 603, reward: 186.0, avg: 155.4\n",
      "episode: 604, reward: 200.0, avg: 157.2\n",
      "episode: 605, reward: 200.0, avg: 158.8\n",
      "episode: 606, reward: 200.0, avg: 160.5\n",
      "episode: 607, reward: 200.0, avg: 162.1\n",
      "episode: 608, reward: 200.0, avg: 163.9\n",
      "episode: 609, reward: 191.0, avg: 165.3\n",
      "episode: 610, reward: 200.0, avg: 166.2\n",
      "episode: 611, reward: 198.0, avg: 167.8\n",
      "episode: 612, reward: 188.0, avg: 169.0\n",
      "episode: 613, reward: 200.0, avg: 170.8\n",
      "episode: 614, reward: 200.0, avg: 171.4\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-8fcd0c0a8ddb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     98\u001b[0m         \u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate_next\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     99\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 100\u001b[1;33m         \u001b[0mupdate_Q\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    102\u001b[0m         \u001b[1;31m# update target network\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1-8fcd0c0a8ddb>\u001b[0m in \u001b[0;36mupdate_Q\u001b[1;34m()\u001b[0m\n\u001b[0;32m     48\u001b[0m                 \u001b[0mtarget\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreward\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mdiscount\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mQ_target\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate_next\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 50\u001b[1;33m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtarget\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mQ\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     51\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\openai\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 550\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    551\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1-8fcd0c0a8ddb>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfcQ1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfcQ2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\openai\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 550\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    551\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\openai\\lib\\site-packages\\torch\\nn\\modules\\linear.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 87\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     88\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\openai\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mlinear\u001b[1;34m(input, weight, bias)\u001b[0m\n\u001b[0;32m   1610\u001b[0m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1611\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1612\u001b[1;33m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1613\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mbias\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1614\u001b[0m             \u001b[0moutput\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "from datetime import datetime\n",
    "from collections import deque\n",
    "import os\n",
    "import random\n",
    "import gym\n",
    "import torch\n",
    "from torch.distributions import Categorical\n",
    "from torch.nn import Module, Linear\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class QNetwork(Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc = Linear(4, 48)\n",
    "        self.fcQ1 = Linear(48, 64)\n",
    "        self.fcQ2 = Linear(64, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fcQ1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fcQ2(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "# network and optimizer\n",
    "Q = QNetwork()\n",
    "optimizer = torch.optim.Adam(Q.parameters(), lr=0.0005)\n",
    "\n",
    "# target network\n",
    "Q_target = QNetwork()\n",
    "\n",
    "history = deque(maxlen=1000000)  # replay buffer\n",
    "discount = 0.99  # discount factor gamma\n",
    "\n",
    "def update_Q():\n",
    "    loss = 0\n",
    "\n",
    "    for state, action, state_next, reward, done in random.sample(history, min(32, len(history))):\n",
    "        with torch.no_grad():\n",
    "            if done:\n",
    "                target = reward\n",
    "            else:\n",
    "                target = reward + discount * torch.max(Q_target(state_next))\n",
    "\n",
    "        loss = loss + (target - Q(state)[action])**2\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "\n",
    "# gym environment\n",
    "env = gym.make(\"CartPole-v0\")\n",
    "max_time_steps = 1000\n",
    "\n",
    "# for computing average reward over 100 episodes\n",
    "reward_history = deque(maxlen=100)\n",
    "\n",
    "\n",
    "# for updating target network\n",
    "target_interval = 1000\n",
    "target_counter = 0\n",
    "\n",
    "# training\n",
    "for episode in range(1000):\n",
    "    # sum of accumulated rewards\n",
    "    rewards = 0\n",
    "\n",
    "    # get initial observation\n",
    "    observation = env.reset()\n",
    "    state = torch.tensor(observation, dtype=torch.float32)\n",
    "\n",
    "    # loop until an episode ends\n",
    "    for t in range(1, max_time_steps + 1):\n",
    "        # display current environment\n",
    "        #env.render()\n",
    "\n",
    "        # epsilon greedy policy for current observation\n",
    "        with torch.no_grad():\n",
    "            if random.random() < 0.01:\n",
    "                action = env.action_space.sample()\n",
    "            else:\n",
    "                action = torch.argmax(Q(state)).item()\n",
    "\n",
    "        # get next observation and current reward for the chosen action\n",
    "        observation_next, reward, done, info = env.step(action)\n",
    "        state_next = torch.tensor(observation_next, dtype=torch.float32)\n",
    "\n",
    "        # collect reward\n",
    "        rewards = rewards + reward\n",
    "\n",
    "        # collect a transition\n",
    "        history.append([state, action, state_next, reward, done])\n",
    "\n",
    "        update_Q()\n",
    "\n",
    "        # update target network\n",
    "        target_counter = target_counter + 1\n",
    "        if target_counter % target_interval == 0:\n",
    "            Q_target.load_state_dict(Q.state_dict())\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "        # pass observation to the next step\n",
    "        observation = observation_next\n",
    "        state = state_next\n",
    "\n",
    "    # compute average reward\n",
    "    reward_history.append(rewards)\n",
    "    avg = sum(reward_history) / len(reward_history)\n",
    "    print('episode: {}, reward: {:.1f}, avg: {:.1f}'.format(episode, rewards, avg))\n",
    "\n",
    "env.close()\n",
    "\n",
    "\n",
    "# TEST     \n",
    "episode = 0\n",
    "state = env.reset()     \n",
    "while episode < 10:  # episode loop\n",
    "    env.render()\n",
    "    state = torch.tensor(state, dtype=torch.float32)\n",
    "    action = torch.argmax(Q(state)).item()\n",
    "    next_state, reward, done, info = env.step(action)  # take a random action\n",
    "    state = next_state\n",
    "\n",
    "    if done:\n",
    "        episode = episode + 1\n",
    "        state = env.reset()\n",
    "env.close()     \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
