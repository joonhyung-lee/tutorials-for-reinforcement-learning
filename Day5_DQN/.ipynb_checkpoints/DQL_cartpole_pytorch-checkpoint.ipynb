{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0, reward: 9.0, avg: 9.0\n",
      "episode: 1, reward: 10.0, avg: 9.5\n",
      "episode: 2, reward: 8.0, avg: 9.0\n",
      "episode: 3, reward: 10.0, avg: 9.2\n",
      "episode: 4, reward: 9.0, avg: 9.2\n",
      "episode: 5, reward: 10.0, avg: 9.3\n",
      "episode: 6, reward: 9.0, avg: 9.3\n",
      "episode: 7, reward: 10.0, avg: 9.4\n",
      "episode: 8, reward: 9.0, avg: 9.3\n",
      "episode: 9, reward: 10.0, avg: 9.4\n",
      "episode: 10, reward: 9.0, avg: 9.4\n",
      "episode: 11, reward: 9.0, avg: 9.3\n",
      "episode: 12, reward: 9.0, avg: 9.3\n",
      "episode: 13, reward: 11.0, avg: 9.4\n",
      "episode: 14, reward: 10.0, avg: 9.5\n",
      "episode: 15, reward: 10.0, avg: 9.5\n",
      "episode: 16, reward: 10.0, avg: 9.5\n",
      "episode: 17, reward: 9.0, avg: 9.5\n",
      "episode: 18, reward: 9.0, avg: 9.5\n",
      "episode: 19, reward: 10.0, avg: 9.5\n",
      "episode: 20, reward: 9.0, avg: 9.5\n",
      "episode: 21, reward: 8.0, avg: 9.4\n",
      "episode: 22, reward: 10.0, avg: 9.4\n",
      "episode: 23, reward: 11.0, avg: 9.5\n",
      "episode: 24, reward: 9.0, avg: 9.5\n",
      "episode: 25, reward: 10.0, avg: 9.5\n",
      "episode: 26, reward: 10.0, avg: 9.5\n",
      "episode: 27, reward: 10.0, avg: 9.5\n",
      "episode: 28, reward: 9.0, avg: 9.5\n",
      "episode: 29, reward: 10.0, avg: 9.5\n",
      "episode: 30, reward: 9.0, avg: 9.5\n",
      "episode: 31, reward: 9.0, avg: 9.5\n",
      "episode: 32, reward: 8.0, avg: 9.5\n",
      "episode: 33, reward: 10.0, avg: 9.5\n",
      "episode: 34, reward: 9.0, avg: 9.5\n",
      "episode: 35, reward: 10.0, avg: 9.5\n",
      "episode: 36, reward: 10.0, avg: 9.5\n",
      "episode: 37, reward: 9.0, avg: 9.5\n",
      "episode: 38, reward: 8.0, avg: 9.4\n",
      "episode: 39, reward: 9.0, avg: 9.4\n",
      "episode: 40, reward: 9.0, avg: 9.4\n",
      "episode: 41, reward: 9.0, avg: 9.4\n",
      "episode: 42, reward: 10.0, avg: 9.4\n",
      "episode: 43, reward: 10.0, avg: 9.4\n",
      "episode: 44, reward: 9.0, avg: 9.4\n",
      "episode: 45, reward: 8.0, avg: 9.4\n",
      "episode: 46, reward: 10.0, avg: 9.4\n",
      "episode: 47, reward: 8.0, avg: 9.4\n",
      "episode: 48, reward: 10.0, avg: 9.4\n",
      "episode: 49, reward: 9.0, avg: 9.4\n",
      "episode: 50, reward: 9.0, avg: 9.4\n",
      "episode: 51, reward: 8.0, avg: 9.3\n",
      "episode: 52, reward: 10.0, avg: 9.4\n",
      "episode: 53, reward: 9.0, avg: 9.4\n",
      "episode: 54, reward: 10.0, avg: 9.4\n",
      "episode: 55, reward: 10.0, avg: 9.4\n",
      "episode: 56, reward: 10.0, avg: 9.4\n",
      "episode: 57, reward: 10.0, avg: 9.4\n",
      "episode: 58, reward: 8.0, avg: 9.4\n",
      "episode: 59, reward: 9.0, avg: 9.4\n",
      "episode: 60, reward: 12.0, avg: 9.4\n",
      "episode: 61, reward: 9.0, avg: 9.4\n",
      "episode: 62, reward: 8.0, avg: 9.4\n",
      "episode: 63, reward: 8.0, avg: 9.4\n",
      "episode: 64, reward: 8.0, avg: 9.3\n",
      "episode: 65, reward: 10.0, avg: 9.3\n",
      "episode: 66, reward: 9.0, avg: 9.3\n",
      "episode: 67, reward: 10.0, avg: 9.4\n",
      "episode: 68, reward: 10.0, avg: 9.4\n",
      "episode: 69, reward: 10.0, avg: 9.4\n",
      "episode: 70, reward: 9.0, avg: 9.4\n",
      "episode: 71, reward: 10.0, avg: 9.4\n",
      "episode: 72, reward: 8.0, avg: 9.4\n",
      "episode: 73, reward: 9.0, avg: 9.4\n",
      "episode: 74, reward: 10.0, avg: 9.4\n",
      "episode: 75, reward: 10.0, avg: 9.4\n",
      "episode: 76, reward: 9.0, avg: 9.4\n",
      "episode: 77, reward: 11.0, avg: 9.4\n",
      "episode: 78, reward: 10.0, avg: 9.4\n",
      "episode: 79, reward: 8.0, avg: 9.4\n",
      "episode: 80, reward: 10.0, avg: 9.4\n",
      "episode: 81, reward: 10.0, avg: 9.4\n",
      "episode: 82, reward: 9.0, avg: 9.4\n",
      "episode: 83, reward: 11.0, avg: 9.4\n",
      "episode: 84, reward: 10.0, avg: 9.4\n",
      "episode: 85, reward: 8.0, avg: 9.4\n",
      "episode: 86, reward: 9.0, avg: 9.4\n",
      "episode: 87, reward: 9.0, avg: 9.4\n",
      "episode: 88, reward: 9.0, avg: 9.4\n",
      "episode: 89, reward: 9.0, avg: 9.4\n",
      "episode: 90, reward: 10.0, avg: 9.4\n",
      "episode: 91, reward: 9.0, avg: 9.4\n",
      "episode: 92, reward: 10.0, avg: 9.4\n",
      "episode: 93, reward: 10.0, avg: 9.4\n",
      "episode: 94, reward: 9.0, avg: 9.4\n",
      "episode: 95, reward: 9.0, avg: 9.4\n",
      "episode: 96, reward: 9.0, avg: 9.4\n",
      "episode: 97, reward: 10.0, avg: 9.4\n",
      "episode: 98, reward: 10.0, avg: 9.4\n",
      "episode: 99, reward: 9.0, avg: 9.4\n",
      "episode: 100, reward: 9.0, avg: 9.4\n",
      "episode: 101, reward: 8.0, avg: 9.4\n",
      "episode: 102, reward: 9.0, avg: 9.4\n",
      "episode: 103, reward: 9.0, avg: 9.4\n",
      "episode: 104, reward: 12.0, avg: 9.4\n",
      "episode: 105, reward: 10.0, avg: 9.4\n",
      "episode: 106, reward: 9.0, avg: 9.4\n",
      "episode: 107, reward: 8.0, avg: 9.4\n",
      "episode: 108, reward: 10.0, avg: 9.4\n",
      "episode: 109, reward: 9.0, avg: 9.4\n",
      "episode: 110, reward: 9.0, avg: 9.4\n",
      "episode: 111, reward: 9.0, avg: 9.4\n",
      "episode: 112, reward: 8.0, avg: 9.4\n",
      "episode: 113, reward: 9.0, avg: 9.3\n",
      "episode: 114, reward: 9.0, avg: 9.3\n",
      "episode: 115, reward: 10.0, avg: 9.3\n",
      "episode: 116, reward: 9.0, avg: 9.3\n",
      "episode: 117, reward: 9.0, avg: 9.3\n",
      "episode: 118, reward: 10.0, avg: 9.3\n",
      "episode: 119, reward: 10.0, avg: 9.3\n",
      "episode: 120, reward: 9.0, avg: 9.3\n",
      "episode: 121, reward: 10.0, avg: 9.4\n",
      "episode: 122, reward: 9.0, avg: 9.3\n",
      "episode: 123, reward: 10.0, avg: 9.3\n",
      "episode: 124, reward: 9.0, avg: 9.3\n",
      "episode: 125, reward: 10.0, avg: 9.3\n",
      "episode: 126, reward: 10.0, avg: 9.3\n",
      "episode: 127, reward: 10.0, avg: 9.3\n",
      "episode: 128, reward: 10.0, avg: 9.3\n",
      "episode: 129, reward: 10.0, avg: 9.3\n",
      "episode: 130, reward: 9.0, avg: 9.3\n",
      "episode: 131, reward: 10.0, avg: 9.4\n",
      "episode: 132, reward: 9.0, avg: 9.4\n",
      "episode: 133, reward: 8.0, avg: 9.3\n",
      "episode: 134, reward: 10.0, avg: 9.4\n",
      "episode: 135, reward: 9.0, avg: 9.3\n",
      "episode: 136, reward: 9.0, avg: 9.3\n",
      "episode: 137, reward: 9.0, avg: 9.3\n",
      "episode: 138, reward: 10.0, avg: 9.4\n",
      "episode: 139, reward: 9.0, avg: 9.4\n",
      "episode: 140, reward: 9.0, avg: 9.4\n",
      "episode: 141, reward: 9.0, avg: 9.4\n",
      "episode: 142, reward: 10.0, avg: 9.4\n",
      "episode: 143, reward: 9.0, avg: 9.3\n",
      "episode: 144, reward: 10.0, avg: 9.4\n",
      "episode: 145, reward: 9.0, avg: 9.4\n",
      "episode: 146, reward: 10.0, avg: 9.4\n",
      "episode: 147, reward: 8.0, avg: 9.4\n",
      "episode: 148, reward: 8.0, avg: 9.3\n",
      "episode: 149, reward: 9.0, avg: 9.3\n",
      "episode: 150, reward: 10.0, avg: 9.4\n",
      "episode: 151, reward: 10.0, avg: 9.4\n",
      "episode: 152, reward: 8.0, avg: 9.4\n",
      "episode: 153, reward: 10.0, avg: 9.4\n",
      "episode: 154, reward: 10.0, avg: 9.4\n",
      "episode: 155, reward: 8.0, avg: 9.3\n",
      "episode: 156, reward: 9.0, avg: 9.3\n",
      "episode: 157, reward: 9.0, avg: 9.3\n",
      "episode: 158, reward: 9.0, avg: 9.3\n",
      "episode: 159, reward: 10.0, avg: 9.3\n",
      "episode: 160, reward: 10.0, avg: 9.3\n",
      "episode: 161, reward: 10.0, avg: 9.3\n",
      "episode: 162, reward: 10.0, avg: 9.4\n",
      "episode: 163, reward: 9.0, avg: 9.4\n",
      "episode: 164, reward: 9.0, avg: 9.4\n",
      "episode: 165, reward: 8.0, avg: 9.4\n",
      "episode: 166, reward: 10.0, avg: 9.4\n",
      "episode: 167, reward: 11.0, avg: 9.4\n",
      "episode: 168, reward: 11.0, avg: 9.4\n",
      "episode: 169, reward: 10.0, avg: 9.4\n",
      "episode: 170, reward: 10.0, avg: 9.4\n",
      "episode: 171, reward: 9.0, avg: 9.4\n",
      "episode: 172, reward: 10.0, avg: 9.4\n",
      "episode: 173, reward: 10.0, avg: 9.4\n",
      "episode: 174, reward: 8.0, avg: 9.4\n",
      "episode: 175, reward: 8.0, avg: 9.4\n",
      "episode: 176, reward: 8.0, avg: 9.4\n",
      "episode: 177, reward: 10.0, avg: 9.4\n",
      "episode: 178, reward: 9.0, avg: 9.3\n",
      "episode: 179, reward: 11.0, avg: 9.4\n",
      "episode: 180, reward: 9.0, avg: 9.4\n",
      "episode: 181, reward: 11.0, avg: 9.4\n",
      "episode: 182, reward: 10.0, avg: 9.4\n",
      "episode: 183, reward: 10.0, avg: 9.4\n",
      "episode: 184, reward: 10.0, avg: 9.4\n",
      "episode: 185, reward: 10.0, avg: 9.4\n",
      "episode: 186, reward: 9.0, avg: 9.4\n",
      "episode: 187, reward: 11.0, avg: 9.4\n",
      "episode: 188, reward: 8.0, avg: 9.4\n",
      "episode: 189, reward: 11.0, avg: 9.4\n",
      "episode: 190, reward: 9.0, avg: 9.4\n",
      "episode: 191, reward: 8.0, avg: 9.4\n",
      "episode: 192, reward: 10.0, avg: 9.4\n",
      "episode: 193, reward: 10.0, avg: 9.4\n",
      "episode: 194, reward: 10.0, avg: 9.4\n",
      "episode: 195, reward: 11.0, avg: 9.4\n",
      "episode: 196, reward: 9.0, avg: 9.4\n",
      "episode: 197, reward: 9.0, avg: 9.4\n",
      "episode: 198, reward: 10.0, avg: 9.4\n",
      "episode: 199, reward: 10.0, avg: 9.4\n",
      "episode: 200, reward: 9.0, avg: 9.4\n",
      "episode: 201, reward: 8.0, avg: 9.4\n",
      "episode: 202, reward: 10.0, avg: 9.4\n",
      "episode: 203, reward: 10.0, avg: 9.5\n",
      "episode: 204, reward: 9.0, avg: 9.4\n",
      "episode: 205, reward: 8.0, avg: 9.4\n",
      "episode: 206, reward: 10.0, avg: 9.4\n",
      "episode: 207, reward: 9.0, avg: 9.4\n",
      "episode: 208, reward: 11.0, avg: 9.4\n",
      "episode: 209, reward: 9.0, avg: 9.4\n",
      "episode: 210, reward: 8.0, avg: 9.4\n",
      "episode: 211, reward: 8.0, avg: 9.4\n",
      "episode: 212, reward: 10.0, avg: 9.4\n",
      "episode: 213, reward: 9.0, avg: 9.4\n",
      "episode: 214, reward: 10.0, avg: 9.4\n",
      "episode: 215, reward: 8.0, avg: 9.4\n",
      "episode: 216, reward: 8.0, avg: 9.4\n",
      "episode: 217, reward: 9.0, avg: 9.4\n",
      "episode: 218, reward: 10.0, avg: 9.4\n",
      "episode: 219, reward: 8.0, avg: 9.4\n",
      "episode: 220, reward: 8.0, avg: 9.4\n",
      "episode: 221, reward: 10.0, avg: 9.4\n",
      "episode: 222, reward: 10.0, avg: 9.4\n",
      "episode: 223, reward: 10.0, avg: 9.4\n",
      "episode: 224, reward: 8.0, avg: 9.4\n",
      "episode: 225, reward: 11.0, avg: 9.4\n",
      "episode: 226, reward: 9.0, avg: 9.4\n",
      "episode: 227, reward: 10.0, avg: 9.4\n",
      "episode: 228, reward: 11.0, avg: 9.4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 229, reward: 9.0, avg: 9.4\n",
      "episode: 230, reward: 10.0, avg: 9.4\n",
      "episode: 231, reward: 10.0, avg: 9.4\n",
      "episode: 232, reward: 10.0, avg: 9.4\n",
      "episode: 233, reward: 9.0, avg: 9.4\n",
      "episode: 234, reward: 11.0, avg: 9.4\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "from datetime import datetime\n",
    "from collections import deque\n",
    "import os\n",
    "import random\n",
    "import gym\n",
    "import torch\n",
    "from torch.distributions import Categorical\n",
    "from torch.nn import Module, Linear\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class QNetwork(Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc = Linear(4, 48)\n",
    "        self.fcQ1 = Linear(48, 64)\n",
    "        self.fcQ2 = Linear(64, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fcQ1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fcQ2(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "# network and optimizer\n",
    "Q = QNetwork()\n",
    "optimizer = torch.optim.Adam(Q.parameters(), lr=0.0005)\n",
    "\n",
    "# target network\n",
    "Q_target = QNetwork()\n",
    "\n",
    "history = deque(maxlen=1000000)  # replay buffer\n",
    "discount = 0.99  # discount factor gamma\n",
    "\n",
    "def update_Q():\n",
    "    loss = 0\n",
    "\n",
    "    for state, action, state_next, reward, done in random.sample(history, min(32, len(history))):\n",
    "        with torch.no_grad():\n",
    "            if done:\n",
    "                target = reward\n",
    "            else:\n",
    "                target = reward + discount * torch.max(Q_target(state_next))\n",
    "\n",
    "        loss = loss + (target - Q(state)[action])**2\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "\n",
    "# gym environment\n",
    "env = gym.make(\"CartPole-v0\")\n",
    "max_time_steps = 1000\n",
    "\n",
    "# for computing average reward over 100 episodes\n",
    "reward_history = deque(maxlen=100)\n",
    "\n",
    "\n",
    "# for updating target network\n",
    "target_interval = 1000\n",
    "target_counter = 0\n",
    "\n",
    "# training\n",
    "for episode in range(1000):\n",
    "    # sum of accumulated rewards\n",
    "    rewards = 0\n",
    "\n",
    "    # get initial observation\n",
    "    observation = env.reset()\n",
    "    state = torch.tensor(observation, dtype=torch.float32)\n",
    "\n",
    "    # loop until an episode ends\n",
    "    for t in range(1, max_time_steps + 1):\n",
    "        # display current environment\n",
    "        #env.render()\n",
    "\n",
    "        # epsilon greedy policy for current observation\n",
    "        with torch.no_grad():\n",
    "            if random.random() < 0.01:\n",
    "                action = env.action_space.sample()\n",
    "            else:\n",
    "                action = torch.argmax(Q(state)).item()\n",
    "\n",
    "        # get next observation and current reward for the chosen action\n",
    "        observation_next, reward, done, info = env.step(action)\n",
    "        state_next = torch.tensor(observation_next, dtype=torch.float32)\n",
    "\n",
    "        # collect reward\n",
    "        rewards = rewards + reward\n",
    "\n",
    "        # collect a transition\n",
    "        history.append([state, action, state_next, reward, done])\n",
    "\n",
    "        update_Q()\n",
    "\n",
    "        # update target network\n",
    "        target_counter = target_counter + 1\n",
    "        if target_counter % target_interval == 0:\n",
    "            Q_target.load_state_dict(Q.state_dict())\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "        # pass observation to the next step\n",
    "        observation = observation_next\n",
    "        state = state_next\n",
    "\n",
    "    # compute average reward\n",
    "    reward_history.append(rewards)\n",
    "    avg = sum(reward_history) / len(reward_history)\n",
    "    print('episode: {}, reward: {:.1f}, avg: {:.1f}'.format(episode, rewards, avg))\n",
    "\n",
    "env.close()\n",
    "\n",
    "\n",
    "# TEST     \n",
    "episode = 0\n",
    "state = env.reset()     \n",
    "while episode < 10:  # episode loop\n",
    "    env.render()\n",
    "    state = torch.tensor(state, dtype=torch.float32)\n",
    "    action = torch.argmax(Q(state)).item()\n",
    "    next_state, reward, done, info = env.step(action)  # take a random action\n",
    "    state = next_state\n",
    "\n",
    "    if done:\n",
    "        episode = episode + 1\n",
    "        state = env.reset()\n",
    "env.close()     \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
