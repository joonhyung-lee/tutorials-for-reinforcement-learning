{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from collections import deque\n",
    "import os\n",
    "import random\n",
    "import gym\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.reset_default_graph()\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior() \n",
    "\n",
    "\n",
    "# parameters to optimize\n",
    "X = tf.placeholder(tf.float32, [1, self.state_size])\n",
    "label = tf.placeholder(tf.float32, [1, self.action_size])\n",
    "G = tf.placeholder(tf.float32, [])\n",
    "W1 = tf.Variable(tf.random_normal([self.state_size, 20], stddev=0.1))\n",
    "b1 = tf.Variable(tf.zeros([20]))\n",
    "L1 = tf.nn.relu(tf.matmul(self.X, self.W1) + self.b1)\n",
    "W2 = tf.Variable(tf.random_normal([20, self.action_size], stddev=0.1))\n",
    "b2 = tf.Variable(tf.zeros([self.action_size]))\n",
    "output = tf.nn.softmax(tf.matmul(self.L1, self.W2) + self.b2)\n",
    "              \n",
    "self.loss = -tf.reduce_sum(self.label * tf.log(self.output)) * self.G\n",
    "self.train_optimizer = tf.train.AdamOptimizer(self.learning_rate).minimize(self.loss)\n",
    "\n",
    "# initialize tensorflow session\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "\n",
    "replay_memory_size = 100\n",
    "minibatch_size = 20\n",
    "\n",
    "history = deque(maxlen=replay_memory_size)  # replay buffer\n",
    "discount = 0.99  # discount factor gamma\n",
    "\n",
    "def act(state):\n",
    "    # run forward propagation to get softmax probabilities\n",
    "    prob_weights = self.sess.run(self.output, feed_dict = {self.X: state.reshape(1, -1)})\n",
    "    action = np.random.choice(range(len(prob_weights.ravel())), p=prob_weights.ravel())\n",
    "    return action\n",
    "\n",
    "\n",
    "def update_pi():\n",
    "    loss = 0\n",
    "    for state, action, state_next, reward, done in random.sample(history, min(minibatch_size, len(history))):\n",
    "        loss = loss + Q(state)[action] * pi(state)[action].log()\n",
    "    loss = loss/min(minibatch_size, len(history))\n",
    "    Actor_optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    Actor_optimizer.step()    \n",
    "\n",
    "\n",
    "def update_Q():\n",
    "    loss = 0\n",
    "\n",
    "    for state, action, state_next, reward, done in random.sample(history, min(minibatch_size, len(history))):\n",
    "        with torch.no_grad():\n",
    "            if done:\n",
    "                target = reward\n",
    "            else:\n",
    "                target = reward + discount * torch.dot(pi(state_next),Q_target(state_next))\n",
    "\n",
    "        loss = loss + (target - Q(state)[action])**2\n",
    "\n",
    "    loss = loss/min(minibatch_size, len(history))\n",
    "    Q_optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    Q_optimizer.step()\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "# gym environment\n",
    "env = gym.make(\"CartPole-v0\")\n",
    "max_time_steps = 1000\n",
    "\n",
    "\n",
    "# training\n",
    "for episode in range(1000):\n",
    "    # sum of accumulated rewards\n",
    "    rewards = 0\n",
    "\n",
    "    # get initial observation\n",
    "    state = env.reset()\n",
    "\n",
    "    # loop until an episode ends\n",
    "    for t in range(1, max_time_steps + 1):\n",
    "        # display current environment\n",
    "        #env.render()\n",
    "\n",
    "        # epsilon greedy policy for current observation\n",
    "        action = act(state)\n",
    "        \n",
    "        # get next observation and current reward for the chosen action\n",
    "        state_next, reward, done, info = env.step(action)\n",
    "\n",
    "        # collect reward\n",
    "        rewards = rewards + reward\n",
    "\n",
    "        # collect a transition\n",
    "        history.append([state, action, state_next, reward, done])\n",
    "\n",
    "        update_Q()\n",
    "        update_pi()\n",
    "        \n",
    "        #Q_target.load_state_dict(Q.state_dict())\n",
    "        # Soft update\n",
    "        for target_param, param in zip(Q_target.parameters(), Q.parameters()):\n",
    "            target_param.data.copy_(param.data * 0.8 + target_param.data * (1.0 - 0.8))\n",
    "\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "        # pass observation to the next step\n",
    "        observation = observation_next\n",
    "        state = state_next\n",
    "\n",
    "    # compute average reward\n",
    "    print('episode: {}, reward: {:.1f}'.format(episode, rewards))\n",
    "\n",
    "env.close()\n",
    "\n",
    "\n",
    "# TEST     \n",
    "episode = 0\n",
    "state = env.reset()     \n",
    "while episode < 5:  # episode loop\n",
    "    env.render()\n",
    "    state = torch.tensor(state, dtype=torch.float32)\n",
    "    probs = pi(state)\n",
    "    action = torch.multinomial(probs, 1).item()\n",
    "    next_state, reward, done, info = env.step(action)  # take a random action\n",
    "    state = next_state\n",
    "\n",
    "    if done:\n",
    "        episode = episode + 1\n",
    "        state = env.reset()\n",
    "env.close()     \n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
