{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"colab":{"name":"DQL_CNN_atari_pytorch.ipynb","provenance":[]},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"scrolled":true,"id":"i94-LeNTABz9","executionInfo":{"status":"ok","timestamp":1621926761398,"user_tz":-540,"elapsed":16110,"user":{"displayName":"Okyong Choi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXt3H1vBv3hDstzKZEIDl0RZQP9ouHdNHms_-n8g=s64","userId":"02726249487150490039"}}},"source":["#!/usr/bin/env python\n","from collections import deque\n","import os\n","import random\n","import gym\n","import torch\n","from torch.distributions import Categorical\n","import torch.nn.functional as F\n","from IPython.display import clear_output\n","import numpy as np\n","from skimage.color import rgb2gray\n","from skimage.transform import rescale\n","\n","\n","\n","class QNetwork(torch.nn.Module):\n","    def __init__(self, num_frames= 4, num_actions=4):\n","        super(QNetwork, self).__init__()\n","        self.num_frames = num_frames\n","\n","        \n","        # Layers\n","        self.conv1 = torch.nn.Conv2d(\n","            in_channels=self.num_frames,\n","            out_channels=16,\n","            kernel_size=8,\n","            stride=4,\n","            padding=2\n","            )\n","        self.conv2 = torch.nn.Conv2d(\n","            in_channels=16,\n","            out_channels=32,\n","            kernel_size=4,\n","            stride=2,\n","            padding=1\n","            )\n","        self.fc1 = torch.nn.Linear(\n","            in_features=3200,\n","            out_features=256,\n","            )\n","        self.fc2 = torch.nn.Linear(\n","            in_features=256,\n","            out_features=num_actions,\n","            )\n","        \n","        # Activation Functions\n","        self.relu = torch.nn.ReLU()\n","    \n","    def flatten(self, x):\n","        batch_size = x.size()[0]\n","        x = x.view(batch_size, -1)\n","        return x\n","    \n","    def forward(self, x):\n","        \n","        # Forward pass\n","        x = self.relu(self.conv1(x))  # In: (80, 80, 4)  Out: (20, 20, 16)\n","        x = self.relu(self.conv2(x))  # In: (20, 20, 16) Out: (10, 10, 32)\n","        x = self.flatten(x)           # In: (10, 10, 32) Out: (3200,)\n","        x = self.relu(self.fc1(x))    # In: (3200,)      Out: (256,)\n","        x = self.fc2(x)               # In: (256,)       Out: (4,)\n","        \n","        return x    \n","    \n","    \n","    \n","    \n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")    \n","    \n","# gym environment\n","#env = gym.make(\"PongNoFrameskip-v4\")\n","env = gym.make(\"Breakout-v0\")\n","\n","# network and optimizer\n","n_actions = env.action_space.n\n","Q = QNetwork().to(device)\n","optimizer = torch.optim.Adam(Q.parameters(), lr=0.0005)\n","\n","# target network\n","Q_target = QNetwork().to(device)\n","Q_target.load_state_dict(Q.state_dict())\n","\n","\n","\n","\n","history = deque(maxlen=100000)  # replay buffer\n","discount = 0.99  # discount factor gamma\n","\n","def update_Q(Q,Q_target,optimizer):\n","    loss = 0\n","\n","    for state, action, state_next, reward, done in random.sample(history, min(32, len(history))):\n","        with torch.no_grad():\n","            if done:\n","                target = reward\n","            else:\n","                target = reward + discount * torch.max(Q_target(state_next.to(device)))\n","        loss = loss + (target - Q(state.to(device))[0][action])**2\n","\n","    optimizer.zero_grad()\n","    loss.backward()\n","    optimizer.step()\n","\n","def process(state):\n","    state = rgb2gray(state[35:195, :, :])\n","    state = rescale(state, scale=0.5)\n","    state = state[np.newaxis, np.newaxis, :, :]\n","    state= state[0][0]\n","    return state \n","    \n","\n","\n"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":false,"colab":{"base_uri":"https://localhost:8080/"},"id":"tRXl_nw_AB0A","outputId":"0f6f5fad-6e81-4c49-9eff-40b1514d5a55"},"source":["max_time_steps = 1000\n","\n","# for computing average reward over 100 episodes\n","reward_history = deque(maxlen=100)\n","\n","\n","# for updating target network\n","target_interval = 1000\n","target_counter = 0\n","\n","# training\n","for episode in range(1500):\n","    # sum of accumulated rewards\n","    rewards = 0\n","\n","    # get initial observation\n","    observation = env.reset()\n","    state =process(observation)\n","    stack = [state] * 4\n","    state = np.array(stack)\n","    state = torch.from_numpy(state).float().to(device).unsqueeze(0)\n","\n","\n","    \n","\n","    # loop until an episode ends\n","    for t in range(1, max_time_steps + 1):\n","        # display current environment\n","        #env.render()\n","        \n","\n","        # epsilon greedy policy for current observation\n","        with torch.no_grad():\n","            if random.random() < 0.05:\n","                action = env.action_space.sample()\n","            else:\n","                #action = Q(state.float().to(device)).max(1)[1].view(1, 1).item()\n","                q_values = Q(state.to(device)).detach()\n","                action = torch.argmax(q_values)\n","        # get next observation and current reward for the chosen action\n","        observation_next, reward, done, info = env.step(action)\n","        state_next =process(observation_next)\n","        stack.pop(0)\n","        stack.append(state_next)\n","        state_next = np.array(stack)\n","        state_next = torch.from_numpy(state_next).float().to(device).unsqueeze(0)\n","\n","        # collect reward\n","        rewards = rewards + reward\n","\n","        # collect a transition\n","        history.append([state, action, state_next, reward, done])\n","\n","        update_Q(Q,Q_target,optimizer)\n","\n","        # update target network\n","        target_counter = target_counter + 1\n","        if target_counter % target_interval == 0:\n","            Q_target.load_state_dict(Q.state_dict())\n","\n","        if done:\n","            env.close()\n","            break\n","\n","        # pass observation to the next step\n","        observation = observation_next\n","        state = state_next\n","\n","    # compute average reward\n","    reward_history.append(rewards)\n","    avg = sum(reward_history) / len(reward_history)\n","    print('episode: {}, reward: {:.1f}, avg: {:.1f}'.format(episode, rewards, avg))\n","\n","env.close()\n","\n","\n","if not os.path.exists(\"./param\"):\n","    os.makedirs(\"./param\")\n","torch.save(Q.state_dict(), 'param/Q_net_params.pkl')\n","\n","\n","\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["episode: 0, reward: 2.0, avg: 2.0\n","episode: 1, reward: 2.0, avg: 2.0\n","episode: 2, reward: 1.0, avg: 1.7\n","episode: 3, reward: 0.0, avg: 1.2\n","episode: 4, reward: 1.0, avg: 1.2\n","episode: 5, reward: 0.0, avg: 1.0\n","episode: 6, reward: 1.0, avg: 1.0\n","episode: 7, reward: 3.0, avg: 1.2\n","episode: 8, reward: 2.0, avg: 1.3\n","episode: 9, reward: 2.0, avg: 1.4\n","episode: 10, reward: 3.0, avg: 1.5\n","episode: 11, reward: 1.0, avg: 1.5\n","episode: 12, reward: 0.0, avg: 1.4\n","episode: 13, reward: 0.0, avg: 1.3\n","episode: 14, reward: 1.0, avg: 1.3\n","episode: 15, reward: 2.0, avg: 1.3\n","episode: 16, reward: 2.0, avg: 1.4\n","episode: 17, reward: 0.0, avg: 1.3\n","episode: 18, reward: 0.0, avg: 1.2\n","episode: 19, reward: 0.0, avg: 1.1\n","episode: 20, reward: 1.0, avg: 1.1\n","episode: 21, reward: 0.0, avg: 1.1\n","episode: 22, reward: 1.0, avg: 1.1\n","episode: 23, reward: 1.0, avg: 1.1\n","episode: 24, reward: 0.0, avg: 1.0\n","episode: 25, reward: 0.0, avg: 1.0\n","episode: 26, reward: 1.0, avg: 1.0\n","episode: 27, reward: 5.0, avg: 1.1\n","episode: 28, reward: 0.0, avg: 1.1\n","episode: 29, reward: 1.0, avg: 1.1\n","episode: 30, reward: 3.0, avg: 1.2\n","episode: 31, reward: 4.0, avg: 1.2\n","episode: 32, reward: 0.0, avg: 1.2\n","episode: 33, reward: 1.0, avg: 1.2\n","episode: 34, reward: 1.0, avg: 1.2\n","episode: 35, reward: 1.0, avg: 1.2\n","episode: 36, reward: 1.0, avg: 1.2\n","episode: 37, reward: 0.0, avg: 1.2\n","episode: 38, reward: 1.0, avg: 1.2\n","episode: 39, reward: 0.0, avg: 1.1\n","episode: 40, reward: 0.0, avg: 1.1\n","episode: 41, reward: 1.0, avg: 1.1\n","episode: 42, reward: 1.0, avg: 1.1\n","episode: 43, reward: 2.0, avg: 1.1\n","episode: 44, reward: 0.0, avg: 1.1\n","episode: 45, reward: 0.0, avg: 1.1\n","episode: 46, reward: 1.0, avg: 1.1\n","episode: 47, reward: 0.0, avg: 1.0\n","episode: 48, reward: 0.0, avg: 1.0\n","episode: 49, reward: 0.0, avg: 1.0\n","episode: 50, reward: 2.0, avg: 1.0\n","episode: 51, reward: 1.0, avg: 1.0\n","episode: 52, reward: 1.0, avg: 1.0\n","episode: 53, reward: 0.0, avg: 1.0\n","episode: 54, reward: 0.0, avg: 1.0\n","episode: 55, reward: 0.0, avg: 1.0\n","episode: 56, reward: 0.0, avg: 0.9\n","episode: 57, reward: 1.0, avg: 0.9\n","episode: 58, reward: 1.0, avg: 0.9\n","episode: 59, reward: 1.0, avg: 0.9\n","episode: 60, reward: 0.0, avg: 0.9\n","episode: 61, reward: 0.0, avg: 0.9\n","episode: 62, reward: 0.0, avg: 0.9\n","episode: 63, reward: 0.0, avg: 0.9\n","episode: 64, reward: 1.0, avg: 0.9\n","episode: 65, reward: 0.0, avg: 0.9\n","episode: 66, reward: 1.0, avg: 0.9\n","episode: 67, reward: 1.0, avg: 0.9\n","episode: 68, reward: 0.0, avg: 0.9\n","episode: 69, reward: 1.0, avg: 0.9\n","episode: 70, reward: 1.0, avg: 0.9\n","episode: 71, reward: 0.0, avg: 0.9\n","episode: 72, reward: 0.0, avg: 0.8\n","episode: 73, reward: 1.0, avg: 0.9\n","episode: 74, reward: 0.0, avg: 0.8\n","episode: 75, reward: 0.0, avg: 0.8\n","episode: 76, reward: 0.0, avg: 0.8\n","episode: 77, reward: 1.0, avg: 0.8\n","episode: 78, reward: 0.0, avg: 0.8\n","episode: 79, reward: 0.0, avg: 0.8\n","episode: 80, reward: 0.0, avg: 0.8\n","episode: 81, reward: 1.0, avg: 0.8\n","episode: 82, reward: 0.0, avg: 0.8\n","episode: 83, reward: 0.0, avg: 0.8\n","episode: 84, reward: 2.0, avg: 0.8\n","episode: 85, reward: 1.0, avg: 0.8\n","episode: 86, reward: 1.0, avg: 0.8\n","episode: 87, reward: 1.0, avg: 0.8\n","episode: 88, reward: 2.0, avg: 0.8\n","episode: 89, reward: 1.0, avg: 0.8\n","episode: 90, reward: 0.0, avg: 0.8\n","episode: 91, reward: 0.0, avg: 0.8\n","episode: 92, reward: 0.0, avg: 0.8\n","episode: 93, reward: 0.0, avg: 0.8\n","episode: 94, reward: 0.0, avg: 0.8\n","episode: 95, reward: 0.0, avg: 0.8\n","episode: 96, reward: 0.0, avg: 0.8\n","episode: 97, reward: 0.0, avg: 0.7\n","episode: 98, reward: 0.0, avg: 0.7\n","episode: 99, reward: 0.0, avg: 0.7\n","episode: 100, reward: 0.0, avg: 0.7\n","episode: 101, reward: 1.0, avg: 0.7\n","episode: 102, reward: 2.0, avg: 0.7\n","episode: 103, reward: 0.0, avg: 0.7\n","episode: 104, reward: 0.0, avg: 0.7\n","episode: 105, reward: 0.0, avg: 0.7\n","episode: 106, reward: 1.0, avg: 0.7\n","episode: 107, reward: 1.0, avg: 0.7\n","episode: 108, reward: 2.0, avg: 0.7\n","episode: 109, reward: 2.0, avg: 0.7\n","episode: 110, reward: 1.0, avg: 0.7\n","episode: 111, reward: 1.0, avg: 0.7\n","episode: 112, reward: 1.0, avg: 0.7\n","episode: 113, reward: 2.0, avg: 0.7\n","episode: 114, reward: 1.0, avg: 0.7\n","episode: 115, reward: 0.0, avg: 0.7\n","episode: 116, reward: 1.0, avg: 0.7\n","episode: 117, reward: 1.0, avg: 0.7\n","episode: 118, reward: 0.0, avg: 0.7\n","episode: 119, reward: 0.0, avg: 0.7\n","episode: 120, reward: 2.0, avg: 0.7\n","episode: 121, reward: 1.0, avg: 0.7\n","episode: 122, reward: 2.0, avg: 0.7\n","episode: 123, reward: 3.0, avg: 0.7\n","episode: 124, reward: 4.0, avg: 0.8\n","episode: 125, reward: 3.0, avg: 0.8\n","episode: 126, reward: 1.0, avg: 0.8\n","episode: 127, reward: 3.0, avg: 0.8\n","episode: 128, reward: 3.0, avg: 0.8\n","episode: 129, reward: 3.0, avg: 0.8\n","episode: 130, reward: 1.0, avg: 0.8\n","episode: 131, reward: 3.0, avg: 0.8\n","episode: 132, reward: 2.0, avg: 0.8\n","episode: 133, reward: 3.0, avg: 0.8\n","episode: 134, reward: 5.0, avg: 0.9\n","episode: 135, reward: 4.0, avg: 0.9\n","episode: 136, reward: 4.0, avg: 0.9\n","episode: 137, reward: 3.0, avg: 1.0\n","episode: 138, reward: 2.0, avg: 1.0\n","episode: 139, reward: 3.0, avg: 1.0\n","episode: 140, reward: 3.0, avg: 1.0\n","episode: 141, reward: 3.0, avg: 1.1\n","episode: 142, reward: 3.0, avg: 1.1\n","episode: 143, reward: 4.0, avg: 1.1\n","episode: 144, reward: 4.0, avg: 1.1\n","episode: 145, reward: 3.0, avg: 1.2\n","episode: 146, reward: 5.0, avg: 1.2\n","episode: 147, reward: 3.0, avg: 1.2\n","episode: 148, reward: 6.0, avg: 1.3\n","episode: 149, reward: 6.0, avg: 1.4\n","episode: 150, reward: 7.0, avg: 1.4\n","episode: 151, reward: 4.0, avg: 1.4\n","episode: 152, reward: 3.0, avg: 1.4\n","episode: 153, reward: 4.0, avg: 1.5\n","episode: 154, reward: 6.0, avg: 1.6\n","episode: 155, reward: 4.0, avg: 1.6\n","episode: 156, reward: 5.0, avg: 1.6\n","episode: 157, reward: 4.0, avg: 1.7\n","episode: 158, reward: 2.0, avg: 1.7\n","episode: 159, reward: 3.0, avg: 1.7\n","episode: 160, reward: 4.0, avg: 1.7\n","episode: 161, reward: 4.0, avg: 1.8\n","episode: 162, reward: 3.0, avg: 1.8\n","episode: 163, reward: 3.0, avg: 1.8\n","episode: 164, reward: 3.0, avg: 1.9\n","episode: 165, reward: 5.0, avg: 1.9\n","episode: 166, reward: 4.0, avg: 1.9\n","episode: 167, reward: 7.0, avg: 2.0\n","episode: 168, reward: 3.0, avg: 2.0\n","episode: 169, reward: 6.0, avg: 2.1\n","episode: 170, reward: 4.0, avg: 2.1\n","episode: 171, reward: 2.0, avg: 2.1\n","episode: 172, reward: 3.0, avg: 2.2\n","episode: 173, reward: 3.0, avg: 2.2\n","episode: 174, reward: 5.0, avg: 2.2\n","episode: 175, reward: 4.0, avg: 2.3\n","episode: 176, reward: 4.0, avg: 2.3\n","episode: 177, reward: 4.0, avg: 2.3\n","episode: 178, reward: 4.0, avg: 2.4\n","episode: 179, reward: 4.0, avg: 2.4\n","episode: 180, reward: 3.0, avg: 2.5\n","episode: 181, reward: 3.0, avg: 2.5\n","episode: 182, reward: 3.0, avg: 2.5\n","episode: 183, reward: 4.0, avg: 2.5\n","episode: 184, reward: 4.0, avg: 2.6\n","episode: 185, reward: 3.0, avg: 2.6\n","episode: 186, reward: 5.0, avg: 2.6\n","episode: 187, reward: 6.0, avg: 2.7\n","episode: 188, reward: 5.0, avg: 2.7\n","episode: 189, reward: 6.0, avg: 2.8\n","episode: 190, reward: 5.0, avg: 2.8\n","episode: 191, reward: 4.0, avg: 2.8\n","episode: 192, reward: 7.0, avg: 2.9\n","episode: 193, reward: 4.0, avg: 3.0\n","episode: 194, reward: 3.0, avg: 3.0\n","episode: 195, reward: 3.0, avg: 3.0\n","episode: 196, reward: 4.0, avg: 3.0\n","episode: 197, reward: 3.0, avg: 3.1\n","episode: 198, reward: 7.0, avg: 3.1\n","episode: 199, reward: 5.0, avg: 3.2\n","episode: 200, reward: 3.0, avg: 3.2\n","episode: 201, reward: 5.0, avg: 3.3\n","episode: 202, reward: 6.0, avg: 3.3\n","episode: 203, reward: 6.0, avg: 3.4\n","episode: 204, reward: 4.0, avg: 3.4\n","episode: 205, reward: 3.0, avg: 3.4\n","episode: 206, reward: 5.0, avg: 3.5\n","episode: 207, reward: 10.0, avg: 3.6\n","episode: 208, reward: 5.0, avg: 3.6\n","episode: 209, reward: 4.0, avg: 3.6\n","episode: 210, reward: 4.0, avg: 3.6\n","episode: 211, reward: 9.0, avg: 3.7\n","episode: 212, reward: 6.0, avg: 3.8\n","episode: 213, reward: 7.0, avg: 3.8\n","episode: 214, reward: 5.0, avg: 3.9\n","episode: 215, reward: 3.0, avg: 3.9\n","episode: 216, reward: 6.0, avg: 4.0\n","episode: 217, reward: 5.0, avg: 4.0\n","episode: 218, reward: 4.0, avg: 4.0\n","episode: 219, reward: 4.0, avg: 4.1\n","episode: 220, reward: 3.0, avg: 4.1\n","episode: 221, reward: 6.0, avg: 4.1\n","episode: 222, reward: 5.0, avg: 4.2\n","episode: 223, reward: 2.0, avg: 4.2\n","episode: 224, reward: 3.0, avg: 4.1\n","episode: 225, reward: 6.0, avg: 4.2\n","episode: 226, reward: 3.0, avg: 4.2\n","episode: 227, reward: 3.0, avg: 4.2\n","episode: 228, reward: 8.0, avg: 4.2\n","episode: 229, reward: 4.0, avg: 4.2\n","episode: 230, reward: 3.0, avg: 4.3\n","episode: 231, reward: 2.0, avg: 4.3\n","episode: 232, reward: 7.0, avg: 4.3\n","episode: 233, reward: 3.0, avg: 4.3\n","episode: 234, reward: 5.0, avg: 4.3\n","episode: 235, reward: 4.0, avg: 4.3\n","episode: 236, reward: 4.0, avg: 4.3\n","episode: 237, reward: 7.0, avg: 4.3\n","episode: 238, reward: 7.0, avg: 4.4\n","episode: 239, reward: 3.0, avg: 4.4\n","episode: 240, reward: 6.0, avg: 4.4\n","episode: 241, reward: 4.0, avg: 4.4\n","episode: 242, reward: 2.0, avg: 4.4\n","episode: 243, reward: 6.0, avg: 4.5\n","episode: 244, reward: 4.0, avg: 4.5\n","episode: 245, reward: 4.0, avg: 4.5\n","episode: 246, reward: 3.0, avg: 4.4\n","episode: 247, reward: 5.0, avg: 4.5\n","episode: 248, reward: 3.0, avg: 4.4\n","episode: 249, reward: 3.0, avg: 4.4\n","episode: 250, reward: 4.0, avg: 4.4\n","episode: 251, reward: 2.0, avg: 4.3\n","episode: 252, reward: 7.0, avg: 4.4\n","episode: 253, reward: 3.0, avg: 4.4\n","episode: 254, reward: 4.0, avg: 4.4\n","episode: 255, reward: 5.0, avg: 4.4\n","episode: 256, reward: 2.0, avg: 4.3\n","episode: 257, reward: 9.0, avg: 4.4\n","episode: 258, reward: 5.0, avg: 4.4\n","episode: 259, reward: 2.0, avg: 4.4\n","episode: 260, reward: 3.0, avg: 4.4\n","episode: 261, reward: 4.0, avg: 4.4\n","episode: 262, reward: 9.0, avg: 4.5\n","episode: 263, reward: 4.0, avg: 4.5\n","episode: 264, reward: 9.0, avg: 4.5\n","episode: 265, reward: 4.0, avg: 4.5\n","episode: 266, reward: 3.0, avg: 4.5\n","episode: 267, reward: 3.0, avg: 4.5\n","episode: 268, reward: 5.0, avg: 4.5\n","episode: 269, reward: 3.0, avg: 4.5\n","episode: 270, reward: 7.0, avg: 4.5\n","episode: 271, reward: 6.0, avg: 4.5\n","episode: 272, reward: 3.0, avg: 4.5\n","episode: 273, reward: 7.0, avg: 4.6\n","episode: 274, reward: 7.0, avg: 4.6\n","episode: 275, reward: 4.0, avg: 4.6\n","episode: 276, reward: 3.0, avg: 4.6\n","episode: 277, reward: 6.0, avg: 4.6\n","episode: 278, reward: 3.0, avg: 4.6\n","episode: 279, reward: 5.0, avg: 4.6\n","episode: 280, reward: 4.0, avg: 4.6\n","episode: 281, reward: 4.0, avg: 4.6\n","episode: 282, reward: 4.0, avg: 4.6\n","episode: 283, reward: 4.0, avg: 4.6\n","episode: 284, reward: 4.0, avg: 4.6\n","episode: 285, reward: 9.0, avg: 4.7\n","episode: 286, reward: 5.0, avg: 4.7\n","episode: 287, reward: 5.0, avg: 4.7\n","episode: 288, reward: 5.0, avg: 4.7\n","episode: 289, reward: 9.0, avg: 4.7\n","episode: 290, reward: 9.0, avg: 4.8\n","episode: 291, reward: 5.0, avg: 4.8\n","episode: 292, reward: 3.0, avg: 4.7\n","episode: 293, reward: 6.0, avg: 4.7\n","episode: 294, reward: 7.0, avg: 4.8\n","episode: 295, reward: 7.0, avg: 4.8\n","episode: 296, reward: 3.0, avg: 4.8\n","episode: 297, reward: 3.0, avg: 4.8\n","episode: 298, reward: 4.0, avg: 4.8\n","episode: 299, reward: 6.0, avg: 4.8\n","episode: 300, reward: 5.0, avg: 4.8\n","episode: 301, reward: 5.0, avg: 4.8\n","episode: 302, reward: 5.0, avg: 4.8\n","episode: 303, reward: 9.0, avg: 4.8\n","episode: 304, reward: 8.0, avg: 4.9\n","episode: 305, reward: 5.0, avg: 4.9\n","episode: 306, reward: 2.0, avg: 4.9\n","episode: 307, reward: 5.0, avg: 4.8\n","episode: 308, reward: 5.0, avg: 4.8\n","episode: 309, reward: 3.0, avg: 4.8\n","episode: 310, reward: 7.0, avg: 4.8\n","episode: 311, reward: 3.0, avg: 4.8\n","episode: 312, reward: 5.0, avg: 4.8\n","episode: 313, reward: 5.0, avg: 4.7\n","episode: 314, reward: 3.0, avg: 4.7\n","episode: 315, reward: 5.0, avg: 4.7\n","episode: 316, reward: 5.0, avg: 4.7\n","episode: 317, reward: 4.0, avg: 4.7\n","episode: 318, reward: 6.0, avg: 4.7\n","episode: 319, reward: 4.0, avg: 4.7\n","episode: 320, reward: 5.0, avg: 4.8\n","episode: 321, reward: 5.0, avg: 4.8\n","episode: 322, reward: 6.0, avg: 4.8\n","episode: 323, reward: 10.0, avg: 4.8\n","episode: 324, reward: 9.0, avg: 4.9\n","episode: 325, reward: 5.0, avg: 4.9\n","episode: 326, reward: 6.0, avg: 4.9\n","episode: 327, reward: 8.0, avg: 5.0\n","episode: 328, reward: 6.0, avg: 5.0\n","episode: 329, reward: 7.0, avg: 5.0\n","episode: 330, reward: 7.0, avg: 5.0\n","episode: 331, reward: 14.0, avg: 5.1\n","episode: 332, reward: 5.0, avg: 5.1\n","episode: 333, reward: 3.0, avg: 5.1\n","episode: 334, reward: 4.0, avg: 5.1\n","episode: 335, reward: 2.0, avg: 5.1\n","episode: 336, reward: 4.0, avg: 5.1\n","episode: 337, reward: 6.0, avg: 5.1\n","episode: 338, reward: 2.0, avg: 5.0\n","episode: 339, reward: 5.0, avg: 5.0\n","episode: 340, reward: 6.0, avg: 5.0\n","episode: 341, reward: 9.0, avg: 5.1\n","episode: 342, reward: 4.0, avg: 5.1\n","episode: 343, reward: 7.0, avg: 5.1\n","episode: 344, reward: 6.0, avg: 5.2\n","episode: 345, reward: 7.0, avg: 5.2\n","episode: 346, reward: 4.0, avg: 5.2\n","episode: 347, reward: 5.0, avg: 5.2\n","episode: 348, reward: 4.0, avg: 5.2\n","episode: 349, reward: 7.0, avg: 5.2\n","episode: 350, reward: 6.0, avg: 5.3\n","episode: 351, reward: 7.0, avg: 5.3\n","episode: 352, reward: 8.0, avg: 5.3\n","episode: 353, reward: 4.0, avg: 5.3\n","episode: 354, reward: 3.0, avg: 5.3\n","episode: 355, reward: 7.0, avg: 5.3\n","episode: 356, reward: 4.0, avg: 5.4\n","episode: 357, reward: 7.0, avg: 5.3\n","episode: 358, reward: 9.0, avg: 5.4\n","episode: 359, reward: 5.0, avg: 5.4\n","episode: 360, reward: 5.0, avg: 5.4\n","episode: 361, reward: 5.0, avg: 5.4\n","episode: 362, reward: 8.0, avg: 5.4\n","episode: 363, reward: 6.0, avg: 5.5\n","episode: 364, reward: 6.0, avg: 5.4\n","episode: 365, reward: 5.0, avg: 5.4\n","episode: 366, reward: 2.0, avg: 5.4\n","episode: 367, reward: 7.0, avg: 5.5\n","episode: 368, reward: 6.0, avg: 5.5\n","episode: 369, reward: 3.0, avg: 5.5\n","episode: 370, reward: 7.0, avg: 5.5\n","episode: 371, reward: 4.0, avg: 5.5\n","episode: 372, reward: 6.0, avg: 5.5\n","episode: 373, reward: 4.0, avg: 5.5\n","episode: 374, reward: 10.0, avg: 5.5\n","episode: 375, reward: 7.0, avg: 5.5\n","episode: 376, reward: 7.0, avg: 5.5\n","episode: 377, reward: 5.0, avg: 5.5\n","episode: 378, reward: 2.0, avg: 5.5\n","episode: 379, reward: 4.0, avg: 5.5\n","episode: 380, reward: 6.0, avg: 5.5\n","episode: 381, reward: 6.0, avg: 5.6\n","episode: 382, reward: 4.0, avg: 5.6\n","episode: 383, reward: 6.0, avg: 5.6\n","episode: 384, reward: 4.0, avg: 5.6\n","episode: 385, reward: 5.0, avg: 5.5\n","episode: 386, reward: 3.0, avg: 5.5\n","episode: 387, reward: 6.0, avg: 5.5\n","episode: 388, reward: 11.0, avg: 5.6\n","episode: 389, reward: 5.0, avg: 5.5\n","episode: 390, reward: 3.0, avg: 5.5\n","episode: 391, reward: 6.0, avg: 5.5\n","episode: 392, reward: 4.0, avg: 5.5\n","episode: 393, reward: 9.0, avg: 5.5\n","episode: 394, reward: 7.0, avg: 5.5\n","episode: 395, reward: 8.0, avg: 5.5\n","episode: 396, reward: 8.0, avg: 5.6\n","episode: 397, reward: 7.0, avg: 5.6\n","episode: 398, reward: 8.0, avg: 5.7\n","episode: 399, reward: 7.0, avg: 5.7\n","episode: 400, reward: 5.0, avg: 5.7\n","episode: 401, reward: 6.0, avg: 5.7\n","episode: 402, reward: 10.0, avg: 5.8\n","episode: 403, reward: 6.0, avg: 5.7\n","episode: 404, reward: 4.0, avg: 5.7\n","episode: 405, reward: 4.0, avg: 5.7\n","episode: 406, reward: 5.0, avg: 5.7\n","episode: 407, reward: 7.0, avg: 5.7\n","episode: 408, reward: 1.0, avg: 5.7\n","episode: 409, reward: 4.0, avg: 5.7\n","episode: 410, reward: 5.0, avg: 5.7\n","episode: 411, reward: 4.0, avg: 5.7\n","episode: 412, reward: 9.0, avg: 5.7\n","episode: 413, reward: 4.0, avg: 5.7\n","episode: 414, reward: 4.0, avg: 5.7\n","episode: 415, reward: 4.0, avg: 5.7\n","episode: 416, reward: 6.0, avg: 5.7\n","episode: 417, reward: 6.0, avg: 5.7\n","episode: 418, reward: 5.0, avg: 5.7\n","episode: 419, reward: 4.0, avg: 5.7\n","episode: 420, reward: 4.0, avg: 5.7\n","episode: 421, reward: 7.0, avg: 5.7\n","episode: 422, reward: 5.0, avg: 5.7\n","episode: 423, reward: 5.0, avg: 5.7\n","episode: 424, reward: 8.0, avg: 5.7\n","episode: 425, reward: 5.0, avg: 5.7\n","episode: 426, reward: 6.0, avg: 5.7\n","episode: 427, reward: 5.0, avg: 5.6\n","episode: 428, reward: 4.0, avg: 5.6\n","episode: 429, reward: 5.0, avg: 5.6\n","episode: 430, reward: 7.0, avg: 5.6\n","episode: 431, reward: 10.0, avg: 5.6\n","episode: 432, reward: 5.0, avg: 5.6\n","episode: 433, reward: 8.0, avg: 5.6\n","episode: 434, reward: 5.0, avg: 5.6\n","episode: 435, reward: 5.0, avg: 5.7\n","episode: 436, reward: 2.0, avg: 5.6\n","episode: 437, reward: 4.0, avg: 5.6\n","episode: 438, reward: 5.0, avg: 5.6\n","episode: 439, reward: 5.0, avg: 5.6\n","episode: 440, reward: 8.0, avg: 5.7\n","episode: 441, reward: 5.0, avg: 5.6\n","episode: 442, reward: 6.0, avg: 5.6\n","episode: 443, reward: 8.0, avg: 5.7\n","episode: 444, reward: 3.0, avg: 5.6\n","episode: 445, reward: 6.0, avg: 5.6\n","episode: 446, reward: 5.0, avg: 5.6\n","episode: 447, reward: 5.0, avg: 5.6\n","episode: 448, reward: 7.0, avg: 5.7\n","episode: 449, reward: 7.0, avg: 5.7\n","episode: 450, reward: 4.0, avg: 5.6\n","episode: 451, reward: 5.0, avg: 5.6\n","episode: 452, reward: 6.0, avg: 5.6\n","episode: 453, reward: 6.0, avg: 5.6\n","episode: 454, reward: 6.0, avg: 5.6\n","episode: 455, reward: 3.0, avg: 5.6\n","episode: 456, reward: 7.0, avg: 5.6\n","episode: 457, reward: 7.0, avg: 5.6\n","episode: 458, reward: 8.0, avg: 5.6\n","episode: 459, reward: 5.0, avg: 5.6\n","episode: 460, reward: 5.0, avg: 5.6\n","episode: 461, reward: 4.0, avg: 5.6\n","episode: 462, reward: 4.0, avg: 5.6\n","episode: 463, reward: 8.0, avg: 5.6\n","episode: 464, reward: 7.0, avg: 5.6\n","episode: 465, reward: 9.0, avg: 5.6\n","episode: 466, reward: 7.0, avg: 5.7\n","episode: 467, reward: 8.0, avg: 5.7\n","episode: 468, reward: 9.0, avg: 5.7\n","episode: 469, reward: 7.0, avg: 5.8\n","episode: 470, reward: 12.0, avg: 5.8\n","episode: 471, reward: 10.0, avg: 5.9\n","episode: 472, reward: 6.0, avg: 5.9\n","episode: 473, reward: 7.0, avg: 5.9\n","episode: 474, reward: 8.0, avg: 5.9\n","episode: 475, reward: 6.0, avg: 5.9\n","episode: 476, reward: 6.0, avg: 5.9\n","episode: 477, reward: 9.0, avg: 5.9\n","episode: 478, reward: 7.0, avg: 6.0\n","episode: 479, reward: 6.0, avg: 6.0\n","episode: 480, reward: 7.0, avg: 6.0\n","episode: 481, reward: 7.0, avg: 6.0\n","episode: 482, reward: 8.0, avg: 6.0\n","episode: 483, reward: 9.0, avg: 6.1\n","episode: 484, reward: 6.0, avg: 6.1\n","episode: 485, reward: 4.0, avg: 6.1\n","episode: 486, reward: 5.0, avg: 6.1\n","episode: 487, reward: 7.0, avg: 6.1\n","episode: 488, reward: 4.0, avg: 6.0\n","episode: 489, reward: 7.0, avg: 6.1\n","episode: 490, reward: 5.0, avg: 6.1\n","episode: 491, reward: 6.0, avg: 6.1\n","episode: 492, reward: 4.0, avg: 6.1\n","episode: 493, reward: 5.0, avg: 6.0\n","episode: 494, reward: 6.0, avg: 6.0\n","episode: 495, reward: 6.0, avg: 6.0\n","episode: 496, reward: 7.0, avg: 6.0\n","episode: 497, reward: 7.0, avg: 6.0\n","episode: 498, reward: 4.0, avg: 6.0\n","episode: 499, reward: 7.0, avg: 6.0\n","episode: 500, reward: 5.0, avg: 6.0\n","episode: 501, reward: 3.0, avg: 5.9\n","episode: 502, reward: 5.0, avg: 5.9\n","episode: 503, reward: 6.0, avg: 5.9\n","episode: 504, reward: 6.0, avg: 5.9\n","episode: 505, reward: 6.0, avg: 5.9\n","episode: 506, reward: 6.0, avg: 5.9\n","episode: 507, reward: 3.0, avg: 5.9\n","episode: 508, reward: 6.0, avg: 5.9\n","episode: 509, reward: 6.0, avg: 6.0\n","episode: 510, reward: 6.0, avg: 6.0\n","episode: 511, reward: 6.0, avg: 6.0\n","episode: 512, reward: 5.0, avg: 6.0\n","episode: 513, reward: 9.0, avg: 6.0\n","episode: 514, reward: 4.0, avg: 6.0\n","episode: 515, reward: 5.0, avg: 6.0\n","episode: 516, reward: 10.0, avg: 6.0\n","episode: 517, reward: 6.0, avg: 6.0\n","episode: 518, reward: 5.0, avg: 6.0\n","episode: 519, reward: 6.0, avg: 6.1\n","episode: 520, reward: 2.0, avg: 6.0\n","episode: 521, reward: 1.0, avg: 6.0\n","episode: 522, reward: 6.0, avg: 6.0\n","episode: 523, reward: 4.0, avg: 6.0\n","episode: 524, reward: 6.0, avg: 6.0\n","episode: 525, reward: 5.0, avg: 6.0\n","episode: 526, reward: 6.0, avg: 6.0\n","episode: 527, reward: 5.0, avg: 6.0\n","episode: 528, reward: 2.0, avg: 6.0\n","episode: 529, reward: 4.0, avg: 5.9\n","episode: 530, reward: 4.0, avg: 5.9\n","episode: 531, reward: 5.0, avg: 5.9\n","episode: 532, reward: 6.0, avg: 5.9\n","episode: 533, reward: 7.0, avg: 5.9\n","episode: 534, reward: 5.0, avg: 5.9\n","episode: 535, reward: 9.0, avg: 5.9\n","episode: 536, reward: 8.0, avg: 6.0\n","episode: 537, reward: 3.0, avg: 6.0\n","episode: 538, reward: 5.0, avg: 6.0\n","episode: 539, reward: 5.0, avg: 6.0\n","episode: 540, reward: 4.0, avg: 5.9\n","episode: 541, reward: 5.0, avg: 5.9\n","episode: 542, reward: 7.0, avg: 5.9\n","episode: 543, reward: 9.0, avg: 5.9\n","episode: 544, reward: 4.0, avg: 5.9\n","episode: 545, reward: 3.0, avg: 5.9\n","episode: 546, reward: 3.0, avg: 5.9\n","episode: 547, reward: 6.0, avg: 5.9\n","episode: 548, reward: 6.0, avg: 5.9\n","episode: 549, reward: 3.0, avg: 5.8\n","episode: 550, reward: 6.0, avg: 5.9\n","episode: 551, reward: 6.0, avg: 5.9\n","episode: 552, reward: 6.0, avg: 5.9\n","episode: 553, reward: 2.0, avg: 5.8\n","episode: 554, reward: 6.0, avg: 5.8\n","episode: 555, reward: 4.0, avg: 5.8\n","episode: 556, reward: 5.0, avg: 5.8\n","episode: 557, reward: 6.0, avg: 5.8\n","episode: 558, reward: 3.0, avg: 5.8\n","episode: 559, reward: 5.0, avg: 5.8\n","episode: 560, reward: 3.0, avg: 5.8\n","episode: 561, reward: 3.0, avg: 5.7\n","episode: 562, reward: 4.0, avg: 5.7\n","episode: 563, reward: 4.0, avg: 5.7\n","episode: 564, reward: 6.0, avg: 5.7\n","episode: 565, reward: 6.0, avg: 5.7\n","episode: 566, reward: 5.0, avg: 5.6\n","episode: 567, reward: 7.0, avg: 5.6\n","episode: 568, reward: 5.0, avg: 5.6\n","episode: 569, reward: 6.0, avg: 5.6\n","episode: 570, reward: 8.0, avg: 5.5\n","episode: 571, reward: 5.0, avg: 5.5\n","episode: 572, reward: 10.0, avg: 5.5\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"jKxh6BS2AB0B"},"source":["from collections import deque\n","import os\n","import random\n","import gym\n","import torch\n","from torch.distributions import Categorical\n","import torch.nn.functional as F\n","from IPython.display import clear_output\n","import numpy as np\n","from skimage.color import rgb2gray\n","from skimage.transform import rescale\n","from time import sleep\n","\n","\n","\n","class QNetwork(torch.nn.Module):\n","    def __init__(self, num_frames= 4, num_actions=4):\n","        super(QNetwork, self).__init__()\n","        self.num_frames = num_frames\n","\n","        \n","        # Layers\n","        self.conv1 = torch.nn.Conv2d(\n","            in_channels=self.num_frames,\n","            out_channels=16,\n","            kernel_size=8,\n","            stride=4,\n","            padding=2\n","            )\n","        self.conv2 = torch.nn.Conv2d(\n","            in_channels=16,\n","            out_channels=32,\n","            kernel_size=4,\n","            stride=2,\n","            padding=1\n","            )\n","        self.fc1 = torch.nn.Linear(\n","            in_features=3200,\n","            out_features=256,\n","            )\n","        self.fc2 = torch.nn.Linear(\n","            in_features=256,\n","            out_features=num_actions,\n","            )\n","        \n","        # Activation Functions\n","        self.relu = torch.nn.ReLU()\n","    \n","    def flatten(self, x):\n","        batch_size = x.size()[0]\n","        x = x.view(batch_size, -1)\n","        return x\n","    \n","    def forward(self, x):\n","        \n","        # Forward pass\n","        x = self.relu(self.conv1(x))  # In: (80, 80, 4)  Out: (20, 20, 16)\n","        x = self.relu(self.conv2(x))  # In: (20, 20, 16) Out: (10, 10, 32)\n","        x = self.flatten(x)           # In: (10, 10, 32) Out: (3200,)\n","        x = self.relu(self.fc1(x))    # In: (3200,)      Out: (256,)\n","        x = self.fc2(x)               # In: (256,)       Out: (4,)\n","        \n","        return x    \n","    \n","    \n","def process(state):\n","    state = rgb2gray(state[35:195, :, :])\n","    state = rescale(state, scale=0.5)\n","    state = state[np.newaxis, np.newaxis, :, :]\n","    state= state[0][0]\n","    return state \n","    \n","    \n","    \n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")    \n","Q = QNetwork().to(device)\n","Q.load_state_dict(torch.load('param/Q_net_params.pkl'))\n","\n","env = gym.make(\"Breakout-v0\")\n","\n","\n","# TEST     \n","episode = 0\n","state = env.reset()  \n","state =process(state)\n","stack = [state] * 4\n","state = np.array(stack)\n","state = torch.from_numpy(state).float().unsqueeze(0)\n","\n","while episode < 10:  # episode loop\n","    env.render()\n","    \n","    q_values = Q(state.to(device)).detach()\n","    action = torch.argmax(q_values)\n","    next_state, reward, done, info = env.step(action)  # take a random action\n","    next_state =process(next_state)\n","    stack.pop(0)\n","    stack.append(next_state)\n","    next_state = np.array(stack)\n","    next_state = torch.from_numpy(next_state).float().unsqueeze(0)\n","    state = next_state\n","    sleep(0.03)\n","\n","    if done:\n","        episode = episode + 1\n","        state = env.reset()\n","        state =process(state)\n","        stack = [state] * 4\n","        state = np.array(stack)\n","        state = torch.from_numpy(state).float().unsqueeze(0)\n","env.close()     "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GB5rri1VAB0C"},"source":[""],"execution_count":null,"outputs":[]}]}