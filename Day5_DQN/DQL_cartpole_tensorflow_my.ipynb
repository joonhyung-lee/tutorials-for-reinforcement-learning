{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.3"},"colab":{"name":"DQL_cartpole_tensorflow_my.ipynb","provenance":[]}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wIQ8FDxxG8DM","executionInfo":{"status":"ok","timestamp":1621839324057,"user_tz":-540,"elapsed":790059,"user":{"displayName":"Okyong Choi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXt3H1vBv3hDstzKZEIDl0RZQP9ouHdNHms_-n8g=s64","userId":"02726249487150490039"}},"outputId":"8a5c94d0-5c04-498f-fb81-d038aa04edd7"},"source":["import gym\n","import tensorflow as tf\n","import numpy as np\n","import time\n","import random\n","from collections import deque\n","from IPython.display import clear_output\n","\n","tf.compat.v1.reset_default_graph()\n","import tensorflow.compat.v1 as tf\n","tf.disable_v2_behavior() \n","\n","\n","epsilon = 1.0\n","epsilon_min = 0.01\n","decay_rate = 0.005\n","replay_memory = deque(maxlen=1000)\n","batch_size = 20\n","gamma = 0.99\n","\n","S = tf.placeholder(tf.float32, [None, 4])\n","A = tf.placeholder(tf.float32, [None, 1])\n","target = tf.placeholder(tf.float32, [None, 1])\n","act_index = tf.placeholder(tf.int32, [None, 2])\n","\n","# Online variables\n","W1s = tf.Variable(tf.random_normal([4, 10], stddev=0.1))\n","W1a = tf.Variable(tf.random_normal([1, 10], stddev=0.1))\n","b1 = tf.Variable(tf.zeros([10]))\n","L1 = tf.nn.relu(tf.matmul(S, W1s) + tf.matmul(A, W1a) + b1)\n","W2 = tf.Variable(tf.random_normal([10, 1], stddev=0.1))\n","Q_online = tf.matmul(L1, W2)\n","\n","# Target variables\n","W1st = tf.Variable(tf.random_normal([4, 10], stddev=0.1))\n","W1at = tf.Variable(tf.random_normal([1, 10], stddev=0.1))\n","b1t = tf.Variable(tf.zeros([10]))\n","L1t = tf.nn.relu(tf.matmul(S, W1st) + tf.matmul(A, W1at) + b1t)\n","W2t = tf.Variable(tf.random_normal([10, 1], stddev=0.1))\n","Q_target = tf.matmul(L1t, W2t)\n","\n","\n","loss = tf.reduce_mean((target - Q_online) ** 2)\n","optimizer = tf.train.AdamOptimizer(learning_rate=0.01).minimize(loss)\n","\n","sess = tf.Session()\n","sess.run(tf.global_variables_initializer())\n","\n","W1st.load(sess.run(W1s),sess)\n","W1at.load(sess.run(W1a),sess)\n","b1t.load(sess.run(b1),sess)\n","W2t.load(sess.run(W2),sess)\n","\n","\n","\n","env = gym.make(\"CartPole-v0\")\n","\n","counter =0\n","for episode in range(30000):\n","    obs = env.reset()\n","    obs = obs.reshape(-1, 4)\n","    done = False\n","    score = 0\n","    while not done:\n","        counter = counter + 1\n","        if random.random() < epsilon:\n","            action = random.randint(0, 1)\n","        else:\n","           \n","            Q1 = sess.run([Q_online], feed_dict={S: obs, A: [[0]]})\n","            Q2 = sess.run([Q_online], feed_dict={S: obs, A: [[1]]})\n","            action = np.argmax(np.array([Q1, Q2]))\n","\n","        \n","        # do action and get new state, reward and done\n","        new_obs, reward, done, _ = env.step(action)\n","        new_obs = new_obs.reshape(-1, 4)\n","        score = score + reward\n","\n","        if done == True:\n","            terminal = 1\n","        else: \n","            terminal = 0\n","        transition = [obs[0], [action], reward, new_obs[0], terminal]\n","        replay_memory.append(transition)\n","        \n","        obs = new_obs\n","        \n","        \n","        if len(replay_memory) >= batch_size:\n","            train_data = random.sample(replay_memory, batch_size)\n","            state   = [data[0] for data in train_data]\n","            action      = [data[1] for data in train_data]\n","            reward      = np.array([data[2] for data in train_data])\n","            new_state   = [data[3] for data in train_data]\n","            terminal    = np.array([data[4] for data in train_data])\n","            \n","            state = np.stack(state)\n","            new_state = np.stack(new_state)\n","\n","            \n","            q_next1 = sess.run([Q_target], feed_dict={S: new_state, A: [[0]]})\n","            q_next2 = sess.run([Q_target], feed_dict={S: new_state, A: [[1]]})\n","\n","            # set batch target value : r + gamma * max(q-value)\n","            batch_target = reward + gamma * np.max([q_next1, q_next2]) * (1-terminal)\n","            batch_target = batch_target.reshape(-1, 1)\n","            \n","            loss_var, _ = sess.run([loss, optimizer], feed_dict={S: state, A: action, target: batch_target})\n","            \n","            if counter % 5000 == 0:\n","                # Target variable update (soft update)\n","                W1st.load(0.5*sess.run(W1s) + 0.5*sess.run(W1st),sess)\n","                W1at.load(0.5*sess.run(W1a) + 0.5*sess.run(W1at),sess)\n","                b1t.load(0.5*sess.run(b1) + 0.5*sess.run(b1t),sess)\n","                W2t.load(0.5*sess.run(W2) + 0.5*sess.run(W2t),sess)\n","                print('Target updated')\n","                \n","            if episode % 100 == 0:\n","                clear_output(wait=True)\n","                print('Episode: {} Score: {}'.format(episode, score))\n","\n","            if epsilon > epsilon_min:\n","                epsilon = epsilon - decay_rate\n","\n","\n","# TEST     \n","episode = 0\n","state = env.reset()     \n","while episode < 10:  # episode loop\n","    #env.render()\n","    Q1 = sess.run([Q_online], feed_dict={S: obs, A: [[0]]})\n","    Q2 = sess.run([Q_online], feed_dict={S: obs, A: [[1]]})\n","    action = np.argmax(np.array([Q1, Q2]))\n","    \n","    next_state, reward, done, info = env.step(action)  # take a random action\n","    state = next_state\n","\n","    if done:\n","        episode = episode + 1\n","        state = env.reset()\n","env.close()    \n"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Episode: 29900 Score: 8.0\n","Target updated\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"IKOtzGz3G8DP"},"source":[""],"execution_count":null,"outputs":[]}]}